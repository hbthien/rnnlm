{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language model in character level "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk, itertools, re\n",
    "\n",
    "# data = open('data/reddit-comments-2015-08.csv', 'r').read()\n",
    "\n",
    "df = pd.read_csv('data/reddit-comments-2015-08.csv')\n",
    "\n",
    "# with open('data/reddit-comments-2015-08.csv', 'r') as f:\n",
    "#     reader = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "#     string = re.sub(r\"\\\\\", \" \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip()\n",
    "\n",
    "df.body = [clean_str(s.strip()) for s in df.body]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I joined a new league this year and they have\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if read from a plain text file\n",
    "data = open('data/text.txt', 'r').read()\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['m', '?', 'l', 'j', '4', '`', 'S', 'y', '.', 'L', '8', 'E', 'r', 'H', 'k', ' ', 'p', 't', \"'\", 'O', '\\\\', 'v', 'u', 'q', 'D', 'A', 'J', 'Y', 's', 'P', 'd', '6', ')', 'M', 'z', '7', 'i', 'w', 'X', 'N', 'C', 'g', 'c', '5', 'Q', 'F', 'b', '!', 'B', '(', 'K', 'h', '2', '3', '9', 'T', 'o', 'f', ',', '0', 'R', 'U', 'I', 'e', 'G', 'x', 'Z', 'W', 'n', 'a', 'V', '1']\n",
      "Data has 7515268 characters in which 72 are unique\n"
     ]
    }
   ],
   "source": [
    "data = '. '.join(df.body)\n",
    "chars = list(set(data))\n",
    "print(chars)\n",
    "print(\"Data has %d characters in which %d are unique\" %(len(data), len(chars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'m': 0, '?': 1, 'l': 2, 'j': 3, '4': 4, '`': 5, 'S': 6, 'y': 7, '.': 8, 'L': 9, '8': 10, 'E': 11, 'r': 12, 'H': 13, 'k': 14, ' ': 15, 'p': 16, 't': 17, \"'\": 18, 'O': 19, '\\\\': 20, 'v': 21, 'u': 22, 'q': 23, 'D': 24, 'A': 25, 'J': 26, 'Y': 27, 's': 28, 'P': 29, 'd': 30, '6': 31, ')': 32, 'M': 33, 'z': 34, '7': 35, 'i': 36, 'w': 37, 'X': 38, 'N': 39, 'C': 40, 'g': 41, 'c': 42, '5': 43, 'Q': 44, 'F': 45, 'b': 46, '!': 47, 'B': 48, '(': 49, 'K': 50, 'h': 51, '2': 52, '3': 53, '9': 54, 'T': 55, 'o': 56, 'f': 57, ',': 58, '0': 59, 'R': 60, 'U': 61, 'I': 62, 'e': 63, 'G': 64, 'x': 65, 'Z': 66, 'W': 67, 'n': 68, 'a': 69, 'V': 70, '1': 71} \n",
      " {0: 'm', 1: '?', 2: 'l', 3: 'j', 4: '4', 5: '`', 6: 'S', 7: 'y', 8: '.', 9: 'L', 10: '8', 11: 'E', 12: 'r', 13: 'H', 14: 'k', 15: ' ', 16: 'p', 17: 't', 18: \"'\", 19: 'O', 20: '\\\\', 21: 'v', 22: 'u', 23: 'q', 24: 'D', 25: 'A', 26: 'J', 27: 'Y', 28: 's', 29: 'P', 30: 'd', 31: '6', 32: ')', 33: 'M', 34: 'z', 35: '7', 36: 'i', 37: 'w', 38: 'X', 39: 'N', 40: 'C', 41: 'g', 42: 'c', 43: '5', 44: 'Q', 45: 'F', 46: 'b', 47: '!', 48: 'B', 49: '(', 50: 'K', 51: 'h', 52: '2', 53: '3', 54: '9', 55: 'T', 56: 'o', 57: 'f', 58: ',', 59: '0', 60: 'R', 61: 'U', 62: 'I', 63: 'e', 64: 'G', 65: 'x', 66: 'Z', 67: 'W', 68: 'n', 69: 'a', 70: 'V', 71: '1'}\n"
     ]
    }
   ],
   "source": [
    "# create char-to-index, index-to-char lists\n",
    "char_to_index = {c:i for i, c in enumerate(chars)}\n",
    "index_to_char = {i:c for i, c in enumerate(chars)}\n",
    "print(char_to_index, \"\\n\", index_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 200 # nb of neurons in hidden layer\n",
    "seq_length = 25 # number of steps to unroll the RNN for, and this is also nb of chars putting in the input of RNN\n",
    "learning_rate = 1e-1\n",
    "\n",
    "data_size = len(data)\n",
    "vocab_size = len(chars) \n",
    "\n",
    "#init parameters\n",
    "W_hh = np.random.randn(hidden_size, hidden_size) #(H,H)\n",
    "W_xh = np.random.randn(vocab_size, hidden_size) #(D,H)\n",
    "W_hy = np.random.randn(hidden_size, vocab_size) #(H,M), M=D\n",
    "b_h = np.zeros((1,hidden_size)) #(H,)\n",
    "b_y = np.zeros((1,vocab_size)) #(M,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reference from @Karpathy: https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "#\n",
    "#                         [b_h]                                                [b_y]\n",
    "#    w2v                    v                                (h_next)            v\n",
    "#  x --> x_s -> [W_xh] -> [sum] -> h_raw -> [nonlinearity] -> h_s -> [W_hy] -> [sum] -> y_s -> [exp(y[k])/sum(exp(y))] -> p_s\n",
    "#                           ^                                  |\n",
    "#                           '----h_prev------[W_hh]------------'\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_forward_single_step(x, h_prev, W_xh, W_hh, W_hy, b_h, b_y):\n",
    "    \"\"\"\n",
    "    Run the forward pass for a single timestep of a vanilla RNN that uses a tanh\n",
    "    activation function.\n",
    "\n",
    "    The input data has dimension D, the hidden state has dimension H, and we use\n",
    "    a minibatch size of N.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data for this timestep, of shape (N, D).\n",
    "    - h_prev: Hidden state from previous timestep, of shape (N, H)\n",
    "    - W_xh: Weight matrix for input-to-hidden connections, of shape (D, H)\n",
    "    - W_hh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n",
    "    - W_hy: Weight matrix for hidden-to-output connections, of shape (H, M)\n",
    "    - b_h: Biases of shape (H,)\n",
    "    - b_y: Bias of shape (M, )\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - h_next: Next hidden state, of shape (N, H)\n",
    "    - y_s: output of this timestep (N, M)\n",
    "    - cache: Tuple of values needed for the backward pass.\n",
    "    \"\"\"\n",
    "    \n",
    "    h_raw = np.dot(x, W_xh) + np.dot(h_prev, W_hh) + b_h  #(N,D)x(D,H) + (N,H)x(H,H) +(1,H) = (N,H)\n",
    "    h_next = np.tanh(h_raw) #(N, H)\n",
    "    y_s = np.dot(h_next, W_hy) + b_y #(N, H)x(H,M) +(1,M) =(N,M)\n",
    "#     p_s = np.exp(y_s) / np.sum(np.exp(y_s), axis=0)\n",
    "    \n",
    "    cache = (x, h_prev, h_next, W_xh, W_hh, W_hy, b_h, b_y)\n",
    "    return h_next, y_s, cache\n",
    "\n",
    "\n",
    "def rnn_forward(x, h0, W_xh, W_hh, W_hy, b_h, b_y):\n",
    "    \"\"\"\n",
    "    Run a vanilla RNN forward on an entire sequence of data. We assume an input\n",
    "    sequence composed of T vectors (each vector represents a word/char), each of dimension D. \n",
    "    The RNN uses a hidden\n",
    "    size of H, and we work over a minibatch containing N sequences. After running\n",
    "    the RNN forward, we return the hidden states for all timesteps.\n",
    "    Inputs:\n",
    "    - x: Input data for the entire timeseries, of shape (N, T, D).\n",
    "    - h0: Initial hidden state, of shape (N, H)\n",
    "    - W_xh: Weight matrix for input-to-hidden connections, of shape (D, H)\n",
    "    - W_hh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n",
    "    - W_hy: Weight matrix for hidden-to-output connections, of shape (H, M)\n",
    "    - b_h: Biases of shape (H,)\n",
    "    - b_y: Bias of shape (M, )\n",
    "    Returns a tuple of:\n",
    "    - h: Hidden states for the entire timeseries, of shape (N, T, H)\n",
    "    - y: Output of the entire timeseries, of shape (N, T, M)\n",
    "    - cache: Values needed in the backward pass\n",
    "    \"\"\"\n",
    "    N, T, D = x.shape\n",
    "    H, M = W_hy.shape\n",
    "    \n",
    "    h = np.empty((N, T, H))\n",
    "    cache = {}\n",
    "    y_s = np.empty((N, T, M))\n",
    "    \n",
    "    for i in range(T):\n",
    "        if i==0: \n",
    "            h[:, i, :], y_s[:, i, :], cache[i] = rnn_forward_single_step(x, h0, W_xh, W_hh, W_hy, b_h, b_y)\n",
    "        else: \n",
    "            h[:, i, :], y_s[:, i, :], cache[i] = rnn_forward_single_step(x, h[:, i-1, :], W_xh, W_hh, W_hy, b_h, b_y)\n",
    "    \n",
    "    return h, y_s, cache\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference from @Karpathy: https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "#\n",
    "#                         [b_h] (1,H)                                         [b_y] (1,M)\n",
    "#    w2v                    v      (N,H)                     (N,H)   (H,M)       v\n",
    "#  x --> x_s -> [W_xh] -> [sum] -> h_raw -> [nonlinearity] -> h_s -> [W_hy] -> [sum] -> y_s -> [exp(y[k])/sum(exp(y))] -> p_s\n",
    "# (N,D)         (D,H)       ^                                  |                       (N,M)\n",
    "#                           '----h_prev------[W_hh]------------'\n",
    "#                                (N,H)       (H,H)\n",
    "\n",
    "def rnn_backward_single_step (dh_next, dy, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for a single timestep of a vanilla RNN.\n",
    "    Inputs:\n",
    "    - dh_next: Gradient of loss with respect to next hidden state (N, H)\n",
    "    - dy: of shape (N,M)\n",
    "    - cache: Cache object from the forward pass\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradients of input data, of shape (N, D)\n",
    "    - dh_prev: Gradients of previous hidden state, of shape (N, H)\n",
    "    - dWxh: Gradients of input-to-hidden weights, of shape (D, H)\n",
    "    - dWhh: Gradients of hidden-to-hidden weights, of shape (H, H)\n",
    "    - dWhy: Gradients of hidden-to-output weights, of shape (H, M)\n",
    "    - dbh: Gradients of bias vector, of shape (H,)\n",
    "    - dby: Gradients of bias vector, of shape (M,)\n",
    "    \n",
    "    \"\"\"\n",
    "    dx, dh_prev, dWxh, dWhh, dWhy, dbh, dby = None, None, None, None, None, None, None\n",
    "    x, h_prev, h_next, W_xh, W_hh, W_hy, b_h, b_y = cache\n",
    "    \n",
    "    \n",
    "    \n",
    "    dby = np.sum(dy, axis=0) #(1,M)\n",
    "    dWhy = np.dot(h_next.T, dy) #(H,N)x(N,M) = (H,M)\n",
    "    dh = np.dot(dy, Why.T) + dh_next # backprop into h, (N,M)x(M,H)=(N,H)\n",
    "    dh_raw = (1 - h_next ** 2) * dh # note: tanh(x)' = 1 - tanh^2(x), shape=(N,H)\n",
    "    dbh = np.sum(dh_raw, axis=0) #(1,H)\n",
    "    dWxh = np.dot(x.T, dh_raw) # (D,N)x(N,H) = (D,H)\n",
    "    dWhh = np.dot(h_prev.T, dh_raw) # (H,N)x(N,H)=(H,H)\n",
    "    dx = np.dot(dh_raw, W_xh.T) # (N,H)x(H,D) = (N,D)\n",
    "    dh_prev = np.dot(dh_raw, W_hh.T) # (N,H)x(H,H) = (N,H)\n",
    "    \n",
    "    ########\n",
    "#     dpre_actv = (1 - next_h ** 2) * dnext_h         # (N, H)\n",
    "#     dx = dpre_actv.dot(Wx.T)\n",
    "#     dprev_h = dpre_actv.dot(Wh.T) #(N,H)x(H,H)=(N,H)\n",
    "#     dWx = x.T.dot(dpre_actv)\n",
    "#     dWh = prev_h.T.dot(dpre_actv)\n",
    "#     db = np.sum(dpre_actv, 0)\n",
    "    \n",
    "    \n",
    "#     dWhy += np.dot(dy, hs[t].T)\n",
    "#     dby += dy\n",
    "#     dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "#     dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "#     dbh += dhraw\n",
    "#     dWxh += np.dot(dhraw, xs[t].T)\n",
    "#     dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "#     dhnext = np.dot(Whh.T, dhraw)\n",
    "    \n",
    "    return dx, dh_prev, dWxh, dWhh, dWhy, dbh, dby\n",
    "\n",
    "\n",
    "def rnn_backward(dh, dy, cache):\n",
    "    \"\"\"\n",
    "    Compute the backward pass for a vanilla RNN over an entire sequence of data.\n",
    "    Inputs:\n",
    "    - dh: Upstream gradients of all hidden states, of shape (N, T, H)\n",
    "    - dy: Upstream gradients of output, of shape (N, M)\n",
    "    - cache\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient of inputs, of shape (N, T, D)\n",
    "    - dh0: Gradient of initial hidden state, of shape (N, H)\n",
    "    - dWx: Gradient of input-to-hidden weights, of shape (D, H)\n",
    "    - dWh: Gradient of hidden-to-hidden weights, of shape (H, H)\n",
    "    - dbh: Gradient of biases, of shape (H,)\n",
    "    - dby: Gradient of biases, of shape (M,)\n",
    "    \"\"\"\n",
    "    N, T, H =dh.shape\n",
    "    N, M = dy.shape\n",
    "    \n",
    "    \n",
    "    dx = np.empty((N,T,D))\n",
    "    dWxh = np.zeros((D,H))\n",
    "    dWhh = np.zeros((H,H))\n",
    "    dWhy = np.zeros((H,M))\n",
    "    dbh = np.zeros((1,H))\n",
    "    dby = np.zeros((1,M))\n",
    "    dh_prev = 0\n",
    "    \n",
    "    for i in reversed(range(T)):\n",
    "        # Add the current timestep upstream gradient to previous calculated dh\n",
    "        dh_next = dh_prev + dh[:,i,:]\n",
    "        dx[:,i,:], dh_prev, dWxh_temp, dWhh_temp, dWhy_temp, dbh_temp, dby_temp = \\\n",
    "                rnn_backward_single_step(dh_next, dy[:,i,:], cache[i])\n",
    "        dWxh += dWxh_temp\n",
    "        dWhy += dWhy_temp\n",
    "        dWhh += dWhh_temp\n",
    "        dbh += dbh_temp\n",
    "        dby += dby_temp\n",
    "    \n",
    "    dh0 = dh_prev\n",
    "    return dx, dh0, dWxh, dWhh, dWhy, dbh, dby\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def temporal_softmax_loss (y_s, y, mask, verbose=False):\n",
    "    \"\"\"\n",
    "    A temporal version of softmax loss for use in RNNs. We assume that we are\n",
    "    making predictions over a vocabulary of size M for each timestep of a\n",
    "    timeseries of length T, over a minibatch of size N. The input y_s gives SCORES\n",
    "    for all vocabulary elements at all timesteps, and y gives the INDICES of the\n",
    "    ground-truth element at each timestep. We use a cross-entropy loss at each\n",
    "    timestep, summing the loss over all timesteps and averaging across the minibatch.\n",
    "    As an additional complication, we may want to ignore the model output at some\n",
    "    timesteps, since sequences of different length may have been combined into a\n",
    "    minibatch and padded with NULL tokens. The optional mask argument tells us\n",
    "    which elements should contribute to the loss.\n",
    "    Inputs:\n",
    "    - y_s: Input scores, of shape (N, T, M)\n",
    "    - y: Ground-truth indices, of shape (N, T) where each element is in the range\n",
    "         0 <= y[i, t] < M\n",
    "    - mask: Boolean array of shape (N, T) where mask[i, t] tells whether or not\n",
    "      the scores at y_s[i, t] should contribute to the loss.\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving loss\n",
    "    - dy_s: Gradient of loss with respect to scores y_s.\n",
    "    \"\"\"\n",
    "    \n",
    "    N, T, M = y_s.shape\n",
    "\n",
    "    ys_flat = y_s.reshape(N * T, M)\n",
    "    y_flat = y.reshape(N * T)\n",
    "    mask_flat = mask.reshape(N * T)\n",
    "\n",
    "    probs = np.exp(ys_flat - np.max(ys_flat, axis=1, keepdims=True))\n",
    "    probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "    loss = -np.sum(mask_flat * np.log(probs[np.arange(N * T), y_flat])) / N\n",
    "    dys_flat = probs.copy()\n",
    "    dys_flat[np.arange(N * T), y_flat] -= 1\n",
    "    dys_flat /= N\n",
    "    dys_flat *= mask_flat[:, None]\n",
    "\n",
    "    if verbose: print('dys_flat: ', dys_flat.shape)\n",
    "\n",
    "    dy_s = dys_flat.reshape(N, T, M)\n",
    "\n",
    "    return loss, dy_s\n",
    "    \n",
    "\n",
    "    \n",
    "def loss(X, y=None):\n",
    "    \"\"\"\n",
    "    Compute training-time loss.\n",
    "    Inputs:\n",
    "    - X: Input (e.g., image features), of shape (N, D)\n",
    "    - y: Ground-truth texts (e.g., captions); an integer array of shape (N, T) where\n",
    "        each element is in the range 0 <= y[i, t] < M\n",
    "    Returns:\n",
    "        If y is None, then run a test-time forward pass of the model and return:\n",
    "        - scores: Array of shape (N, M) giving classification scores, where\n",
    "          scores[i, c] is the classification score for X[i] and the word/char with index c.\n",
    "\n",
    "        If y is not None, then run a training-time forward and backward pass and\n",
    "        return a tuple of:\n",
    "        - loss: Scalar value giving the loss\n",
    "        - grads: Dictionary of gradients \n",
    "    \"\"\"\n",
    "    \n",
    "    # Cut y into two pieces: text_in has everything but the last word/char\n",
    "    # and will be input to the RNN; text_out has everything but the first\n",
    "    # word/char and this is what we will expect the RNN to generate. These are offset\n",
    "    # by one relative to each other because the RNN should produce word/char (t+1)\n",
    "    # after receiving word/char (t). The first element of text_in will be the START\n",
    "    # token, and the first element of text_out will be the first word/char, \n",
    "    # and the last element of text_out is END_TOKEN\n",
    "        \n",
    "    text_in = y[:, :-1]\n",
    "    text_out = y[:, 1:]\n",
    "    \n",
    "    mask = (text_out != self._null)\n",
    "    \n",
    "    \n",
    "    scores = rnn_forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Build model (classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
