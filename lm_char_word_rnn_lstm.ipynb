{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language model in character/word level "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I joined a new league this year and they have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In your scenario, a person could just not run ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They don't get paid for how much time you spen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I dunno, back before the August update in an A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No, but Toriyama sometimes would draw himself ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body\n",
       "0  I joined a new league this year and they have ...\n",
       "1  In your scenario, a person could just not run ...\n",
       "2  They don't get paid for how much time you spen...\n",
       "3  I dunno, back before the August update in an A...\n",
       "4  No, but Toriyama sometimes would draw himself ..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk, itertools, re\n",
    "import itertools, operator\n",
    "from collections import Counter\n",
    "\n",
    "# data = open('data/reddit-comments-2015-08.csv', 'r').read()\n",
    "\n",
    "df = pd.read_csv('data/reddit-comments-2015-08.csv')\n",
    "\n",
    "# with open('data/reddit-comments-2015-08.csv', 'r') as f:\n",
    "#     reader = \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    i joined a new league this year and they have ...\n",
       "1    in your scenario , a person could just not run...\n",
       "2    they do n't get paid for how much time you spe...\n",
       "3    i dunno , back before the august update in an ...\n",
       "4    no , but toriyama sometimes would draw himself...\n",
       "Name: body, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train = df.body.str.lower()\n",
    "X_train = df.body.map(lambda x: clean_str(x)).head(100)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I joined a new league this year and they have\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if read from a plain text file\n",
    "data = open('data/text.txt', 'r').read()\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Char-level data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', \"'\", '(', ')', ',', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '?', '\\\\', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Data has 7500269 characters in which 46 are unique\n"
     ]
    }
   ],
   "source": [
    "char_data = [clean_str(s.strip()) for s in df.body]\n",
    "char_data = '.'.join(char_data)\n",
    "char_data\n",
    "# #Consider char-level\n",
    "# char_list = \"abcdefghijklmnopqrstuvwxyz0123456789 ,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "\n",
    "chars = list(set(char_data))\n",
    "chars=sorted(chars)\n",
    "print(chars)\n",
    "print(\"Data has %d characters in which %d are unique\" %(len(char_data), len(chars)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'§': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, ',': 6, '.': 7, '0': 8, '1': 9, '2': 10, '3': 11, '4': 12, '5': 13, '6': 14, '7': 15, '8': 16, '9': 17, '?': 18, '\\\\': 19, '`': 20, 'a': 21, 'b': 22, 'c': 23, 'd': 24, 'e': 25, 'f': 26, 'g': 27, 'h': 28, 'i': 29, 'j': 30, 'k': 31, 'l': 32, 'm': 33, 'n': 34, 'o': 35, 'p': 36, 'q': 37, 'r': 38, 's': 39, 't': 40, 'u': 41, 'v': 42, 'w': 43, 'x': 44, 'y': 45, 'z': 46} \n",
      " ['§', ' ', '!', \"'\", '(', ')', ',', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '?', '\\\\', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "2361\n"
     ]
    }
   ],
   "source": [
    "# create char-to-index, index-to-char lists\n",
    "unknown_char = '§'\n",
    "index_to_char = [unknown_char] +[x[0] for x in chars] \n",
    "char_to_index = {c:i for i, c in enumerate(index_to_char)}  \n",
    "print(char_to_index, \"\\n\", index_to_char)\n",
    " \n",
    "#find a maximum length of a row in the X_train\n",
    "#this will be used in the case \"considering each row of data as a 'full' sentence\n",
    "lenchar_of_row=X_train.map(len).max()\n",
    "print(lenchar_of_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [29, 1, 30, 35, 29, 34, 25, 24, 1, 21, 1, 34, ...\n",
      "1    [29, 34, 1, 45, 35, 41, 38, 1, 39, 23, 25, 34,...\n",
      "2    [40, 28, 25, 45, 1, 24, 35, 1, 34, 3, 40, 1, 2...\n",
      "3    [29, 1, 24, 41, 34, 34, 35, 1, 6, 1, 22, 21, 2...\n",
      "4    [34, 35, 1, 6, 1, 22, 41, 40, 1, 40, 35, 38, 2...\n",
      "Name: body, dtype: object\n",
      "<class 'numpy.matrixlib.defmatrix.matrix'> (100, 2361)\n"
     ]
    }
   ],
   "source": [
    "######Convert text into integer ######\n",
    "# Method 1: consider chars of each sentence differently\n",
    "\n",
    "ss = X_train.str.pad(lenchar_of_row, side='right', fillchar=unknown_char).\\\n",
    "        map(lambda st: [char_to_index[c] for c in st])\n",
    "# lenchar_of_row=X_trailencharn_char.map(len).max()\n",
    "# X_train_char = X_train_char.as_matrix()\n",
    "# print(len(X_train_char[33]))\n",
    "\n",
    "print(ss.head())\n",
    "# ss = pd.Series(data = ss)\n",
    "X_train_char = np.matrix(ss.tolist())\n",
    "print(type(X_train_char), X_train_char.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.85 ms ± 463 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "7.23 ms ± 1.48 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "5.05 ms ± 1.12 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "#Just reference to see the speed of finding max\n",
    "%timeit -n 100 df.body.str.len().max()\n",
    "%timeit -n 100 df.body.map(lambda x: len(x)).max()\n",
    "%timeit -n 100 df.body.map(len).max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word-level data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE_START no , but toriyama sometimes would draw himself as a little robot shen was a funny character for a few episodes \\( hitting yamcha in the junk \\) before you find out his true identity then he has an awesome fight with piccolo SENTENCE_END\n"
     ]
    }
   ],
   "source": [
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "#Consider word-level\n",
    "# word_data = [\"%s %s %s\" %(sentence_start_token, s, sentence_end_token) for s in df.body]\n",
    "# word_data[0]\n",
    "\n",
    "def getTokenizedSentences(lines, min_sent_characters=1):\n",
    "    # Split full comments into sentences\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x) for x in lines])\n",
    "    sentences = [clean_str(s) for s in sentences if \"http\" not in s and len(s) >= min_sent_characters]\n",
    "    # Append SENTENCE_START and SENTENCE_END\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "    return sentences\n",
    "\n",
    "sents = getTokenizedSentences(X_train)\n",
    "print(sents[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2126 unique words tokens.\n",
      "Using vocabulary size 8000.\n",
      "The most frequent word in our vocabulary is 'the' and appeared 304 times.\n",
      "The least frequent word in our vocabulary is ''looking' and appeared 1 times.\n",
      "{'<MASK/>': 0, 'UNKNOWN_TOKEN': 1, 'zwarte': 2, 'zul': 3, 'zelf': 4, 'zeer': 5, 'zeal': 6, 'ze': 7, 'youtube': 8, 'youthful': 9, 'young': 10, 'yorick': 11, 'yes': 12, 'yelling': 13, 'yea': 14, 'yards': 15, 'yardage': 16, 'yamcha': 17, 'yac': 18, 'wrong': 19, 'writer': 20, 'write': 21, 'wr': 22, 'world': 23, 'worked': 24, 'worden': 25, 'woohoo': 26, 'won': 27, 'woman': 28, 'wolf': 29, 'within': 30, 'withdrew': 31, 'withdrawn': 32, 'withdrawals': 33, 'wish': 34, 'wiring': 35, 'wires': 36, 'wire': 37, 'wine': 38, 'wiling': 39, 'wildly': 40, 'wildest': 41, 'whoever': 42, 'whistling': 43, 'whispering': 44, 'whichever': 45, 'wheels': 46, 'whatever': 47, 'west': 48, 'werkgroep': 49, 'weird': 50, 'weight': 51, 'weekly': 52, 'website': 53, 'weather': 54, 'weapon': 55, 'wealthy': 56, 'weaker': 57, 'weak': 58, 'wc': 59, 'watched': 60, 'watch': 61, 'wasted': 62, 'warnings': 63, 'warn': 64, 'warmer': 65, 'waiting': 66, 'waar': 67, 'vorm': 68, 'voluntary': 69, 'volume': 70, 'vinden': 71, 'villain': 72, 'vile': 73, 'viewed': 74, 'view': 75, 'videos': 76, 'vidal': 77, 'victim': 78, 'verratti': 79, 'verified': 80, 'verene': 81, 'vaxx': 82, 'vastly': 83, 'variation': 84, 'values': 85, 'value': 86, 'utmost': 87, 'usurpations': 88, 'usually': 89, 'uses': 90, 'username': 91, 'useless': 92, 'upset': 93, 'upon': 94, 'updated': 95, 'update': 96, 'unstoppable': 97, 'unrelated': 98, 'unpopular': 99, 'unneeded': 100, 'unlocked': 101, 'unless': 102, 'unique': 103, 'unions': 104, 'unimpeachable': 105, 'uni': 106, 'unfounded': 107, 'unfamiliar': 108, 'unexpierenced': 109, 'understanding': 110, 'underpowered': 111, 'unalienable': 112, 'ultimatums': 113, 'typical': 114, 'types': 115, 'two': 116, 'tweaks': 117, 'tutorials': 118, 'tutorial': 119, 'turns': 120, 'truths': 121, 'truth': 122, 'trump': 123, 'truly': 124, 'trudeau': 125, 'truck': 126, 'trouble': 127, 'trooper': 128, 'troggelen': 129, 'trips': 130, 'trigger': 131, 'tries': 132, 'trial': 133, 'trees': 134, 'treated': 135, 'treacherous': 136, 'transient': 137, 'train': 138, 'trailers': 139, 'tradition': 140, 'traction': 141, 'track': 142, 'traced': 143, 'town': 144, 'towing': 145, 'tower': 146, 'towards': 147, 'tow': 148, 'totals': 149, 'totally': 150, 'total': 151, 'tossing': 152, 'toss': 153, 'toriyama': 154, 'tor': 155, 'topic': 156, 'tool': 157, 'took': 158, 'tone': 159, 'tl': 160, 'tis': 161, 'tires': 162, 'tire': 163, 'tip': 164, 'till': 165, 'ticket': 166, 'thus': 167, 'thrust': 168, 'throw': 169, 'threw': 170, 'things': 171, 'thimerosal': 172, 'thier': 173, 'theyve': 174, 'thermal': 175, 'theory': 176, 'themselves': 177, 'theme': 178, 'thematically': 179, 'thats': 180, 'thanks': 181, 'terrifying': 182, 'terms': 183, 'tenured': 184, 'tenure': 185, 'tells': 186, 'tellingly': 187, 'technical': 188, 'tech': 189, 'teammates': 190, 'tea': 191, 'te': 192, 'td': 193, 'tbh': 194, 'taxed': 195, 'taxable': 196, 'targeted': 197, 'tampa': 198, 'tale': 199, 'takes': 200, 'taken': 201, 'tag': 202, 'table': 203, 't': 204, 'swooping': 205, 'swimming': 206, 'sweeping': 207, 'suspiciously': 208, 'suspense': 209, 'survived': 210, 'surveillance': 211, 'supposed': 212, 'supporting': 213, 'supported': 214, 'supplement': 215, 'suggestions': 216, 'suggesting': 217, 'suggested': 218, 'suggest': 219, 'sufferable': 220, 'sudden': 221, 'suction': 222, 'succeed': 223, 'subsequent': 224, 'subscriber': 225, 'subreddits': 226, 'subject': 227, 'sub': 228, 'style': 229, 'study': 230, 'struggling': 231, 'structured': 232, 'strongly': 233, 'strip': 234, 'stretched': 235, 'street': 236, 'strategy': 237, 'straight': 238, 'storyline': 239, 'story': 240, 'storing': 241, 'stories': 242, 'stored': 243, 'store': 244, 'stick': 245, 'stereotype': 246, 'stepping': 247, 'stepped': 248, 'staying': 249, 'stay': 250, 'status': 251, 'station': 252, 'stat': 253, 'starts': 254, 'staring': 255, 'star': 256, 'stadium': 257, 'square': 258, 'spree': 259, 'spotify': 260, 'spot': 261, 'spoiled': 262, 'splat': 263, 'splash': 264, 'spies': 265, 'spied': 266, 'spice': 267, 'sphere': 268, 'speedy': 269, 'speedlings': 270, 'specifics': 271, 'speak': 272, 'southeast': 273, 'south': 274, 'source': 275, 'sounds': 276, 'somewhere': 277, 'somewhat': 278, 'someday': 279, 'solid': 280, 'smokes': 281, 'smiled': 282, 'smaller': 283, 'slumping': 284, 'slim': 285, 'slightly': 286, 'slavernijverleden': 287, 'slacker': 288, 'skyrim': 289, 'skipping': 290, 'skip': 291, 'skin': 292, 'sjw': 293, 'sizes': 294, 'six': 295, 'site': 296, 'singing': 297, 'silence': 298, 'significant': 299, 'signature': 300, 'sight': 301, 'sick': 302, 'shown': 303, 'shewn': 304, 'shepherd': 305, 'shen': 306, 'sheltered': 307, 'shattuck': 308, 'shall': 309, 'sex': 310, 'seven': 311, 'settings': 312, 'session': 313, 'serve': 314, 'seriously': 315, 'serieuze': 316, 'serie': 317, 'send': 318, 'semifinals': 319, 'self': 320, 'seemed': 321, 'seeing': 322, 'security': 323, 'secure': 324, 'seconds': 325, 'seats': 326, 'seat': 327, 'search': 328, 'seaglass': 329, 'scrolls': 330, 'script': 331, 'scraped': 332, 'scoring': 333, 'scientifically': 334, 'schein': 335, 'scenario': 336, 'saying': 337, 'saves': 338, 'savage': 339, 'sanders': 340, 'sand': 341, 'sanctimoniously': 342, 'saltspray': 343, 'sales': 344, 'safety': 345, 'ruling': 346, 'rude': 347, 'round': 348, 'rough': 349, 'rotting': 350, 'roster': 351, 'roosh': 352, 'romantically': 353, 'rolled': 354, 'rok': 355, 'robot': 356, 'rng': 357, 'risking': 358, 'rings': 359, 'rig': 360, 'rider': 361, 'rework': 362, 'reviewed': 363, 'reveal': 364, 'retard': 365, 'respond': 366, 'respective': 367, 'respect': 368, 'resourse': 369, 'resolution': 370, 'resistance': 371, 'resident': 372, 'reserving': 373, 'requires': 374, 'requirements': 375, 'requests': 376, 'republicans': 377, 'republican': 378, 'reproducible': 379, 'repressed': 380, 'reports': 381, 'replaced': 382, 'repeat': 383, 'removes': 384, 'removal': 385, 'rely': 386, 'reloaded': 387, 'reliably': 388, 'relative': 389, 'related': 390, 'rejiggering': 391, 'regular': 392, 'regering': 393, 'regarding': 394, 'refuse': 395, 'reform': 396, 'referred': 397, 'reduction': 398, 'redone': 399, 'red': 400, 'recognize': 401, 'recognition': 402, 'reckless': 403, 'receiving': 404, 'receive': 405, 'rec': 406, 'rebalance': 407, 'reactors': 408, 'reaction': 409, 'react': 410, 'reach': 411, 'rcs': 412, 'rb': 413, 'rate': 414, 'rapport': 415, 'rapes': 416, 'rape': 417, 'rannvaig': 418, 'ranking': 419, 'rank': 420, 'ranged': 421, 'rakitic': 422, 'raising': 423, 'rag': 424, 'radio': 425, 'radically': 426, 'rad': 427, 'racisme': 428, 'quick': 429, 'quests': 430, 'questionable': 431, 'question': 432, 'queensland': 433, 'queen': 434, 'qualify': 435, 'qbs': 436, 'puts': 437, 'pursuit': 438, 'pursuing': 439, 'purely': 440, 'punishment': 441, 'punished': 442, 'ptsd': 443, 'ps4': 444, 'prudence': 445, 'provides': 446, 'provided': 447, 'prototype': 448, 'protected': 449, 'prosecutors': 450, 'proposal': 451, 'property': 452, 'promoted': 453, 'promise': 454, 'project': 455, 'progressive': 456, 'programme': 457, 'program': 458, 'profits': 459, 'profiles': 460, 'profile': 461, 'products': 462, 'produced': 463, 'procedure': 464, 'problems': 465, 'probeerde': 466, 'pro': 467, 'privatisations': 468, 'prioma': 469, 'printed': 470, 'principles': 471, 'priestess': 472, 'priest': 473, 'prevent': 474, 'prettig': 475, 'presume': 476, 'pressure': 477, 'presented': 478, 'preparing': 479, 'premade': 480, 'prefer': 481, 'predominantly': 482, 'precise': 483, 'pray': 484, 'potential': 485, 'posted': 486, 'possibility': 487, 'positives': 488, 'position': 489, 'portions': 490, 'popular': 491, 'pools': 492, 'politics': 493, 'political': 494, 'pointed': 495, 'pogba': 496, 'plugins': 497, 'plot': 498, 'playstation': 499, 'plays': 500, 'plaintiffs': 501, 'plaintiff': 502, 'pizzazz': 503, 'pistons': 504, 'pirate': 505, 'pill': 506, 'pieters': 507, 'piet': 508, 'pieces': 509, 'piece': 510, 'pickups': 511, 'pick': 512, 'piccolo': 513, 'physically': 514, 'phone': 515, 'philosophy': 516, 'philosophies': 517, 'phase': 518, 'pgh311': 519, 'peter': 520, 'perhaps': 521, 'performance': 522, 'perfect': 523, 'percent': 524, 'perceived': 525, 'pep': 526, 'pellar': 527, 'pedophilia': 528, 'pays': 529, 'paying': 530, 'patrick': 531, 'password': 532, 'pass': 533, 'party': 534, 'particularly': 535, 'parliament': 536, 'paraded': 537, 'pansy': 538, 'panel': 539, 'pandora': 540, 'painted': 541, 'packer': 542, 'pace': 543, 'p90d': 544, 'p': 545, 'oxenfurt': 546, 'owner': 547, 'overwhelming': 548, 'overreact': 549, 'overloads': 550, 'outs': 551, 'outright': 552, 'outline': 553, 'outlet': 554, 'outlefting': 555, 'otherwise': 556, 'origin': 557, 'organizing': 558, 'ordering': 559, 'opposite': 560, 'opposes': 561, 'opportunities': 562, 'operate': 563, 'opened': 564, 'ontario': 565, 'online': 566, 'ongoing': 567, 'onderbuikgevoel': 568, 'once': 569, 'onbelangrijks': 570, 'om': 571, 'olive': 572, 'older': 573, 'oil': 574, 'often': 575, 'office': 576, 'occasion': 577, 'obviously': 578, 'obvious': 579, 'obligation': 580, 'obligated': 581, 'objections': 582, 'object': 583, \"o'mac\": 584, 'o': 585, 'numbers': 586, 'number': 587, 'nullification': 588, 'nsa': 589, 'notorious': 590, 'noticed': 591, 'northeast': 592, 'normal': 593, 'nope': 594, 'node': 595, 'nodded': 596, 'nightmare': 597, 'nics': 598, 'nice': 599, 'next': 600, 'netherlands': 601, 'nervousness': 602, 'nepali': 603, 'neoliberal': 604, 'negima': 605, 'needy': 606, 'nederlandse': 607, 'neat': 608, 'naughty': 609, 'nations': 610, 'narratve': 611, 'narrator': 612, 'names': 613, 'namelijk': 614, 'nah': 615, 'mysteries': 616, 'mutilated': 617, 'muscle': 618, 'mrm': 619, 'mpps': 620, 'movie': 621, 'move': 622, 'mother': 623, 'moron': 624, 'monarchy': 625, 'mom': 626, 'molest': 627, 'model': 628, 'misunderstood': 629, 'missing': 630, 'misinformation': 631, 'minutes': 632, 'mini': 633, 'mine': 634, 'millions': 635, 'mileage': 636, 'might': 637, 'midwest': 638, 'midfielders': 639, 'midfield': 640, 'middle': 641, 'michael': 642, 'mexico': 643, 'methodical': 644, 'meta': 645, 'met': 646, 'messaging': 647, 'merken': 648, 'members': 649, 'melbourne': 650, 'meedoet': 651, 'medication': 652, 'medical': 653, 'mechanic': 654, 'meanwhile': 655, 'material': 656, 'matches': 657, 'mass': 658, 'marriage': 659, 'marketing': 660, 'market': 661, 'marchisio': 662, 'manufactures': 663, 'maneuvers': 664, 'management': 665, 'managed': 666, 'manage': 667, 'man': 668, 'makeup': 669, 'majority': 670, 'major': 671, 'mainly': 672, 'magic': 673, 'm': 674, 'lunch': 675, 'lumped': 676, 'lp': 677, 'lower': 678, 'love': 679, 'lost': 680, 'lorre': 681, 'looked': 682, 'longer': 683, 'lol': 684, 'logical': 685, 'location': 686, 'lobby': 687, 'lives': 688, 'lists': 689, 'listened': 690, 'links': 691, 'linkedin': 692, 'linked': 693, 'lines': 694, 'line': 695, 'limited': 696, 'lightning': 697, 'lifer': 698, 'liberty': 699, 'liberals': 700, 'liability': 701, 'let': 702, 'lesnar': 703, 'lens': 704, 'legit': 705, 'leftist': 706, 'leest': 707, 'leaves': 708, 'leave': 709, 'learning': 710, 'learned': 711, 'leaf': 712, 'leads': 713, 'leaders': 714, 'layout': 715, 'laying': 716, 'laws': 717, 'law': 718, 'laughable': 719, 'latter': 720, 'latest': 721, 'lastly': 722, 'lang': 723, 'lanes': 724, 'lane': 725, 'korean': 726, 'kongolo': 727, 'known': 728, 'knowledgeable': 729, 'knowing': 730, 'killers': 731, 'kicking': 732, 'kicked': 733, 'key': 734, 'kept': 735, 'keira': 736, 'keeping': 737, 'junk': 738, 'jumps': 739, 'jumper': 740, 'jonah': 741, 'joined': 742, 'janmaat': 743, 'ive': 744, 'itself': 745, 'italy': 746, 'italian': 747, 'isnt': 748, 'irregardless': 749, 'involvement': 750, 'investment': 751, 'inventors': 752, 'invariably': 753, 'interrupted': 754, 'intermittent': 755, 'integral': 756, 'intact': 757, 'instituted': 758, 'institute': 759, 'instances': 760, 'installing': 761, 'innovation': 762, 'innocent': 763, 'init': 764, 'iniesta': 765, 'infuriating': 766, 'influence': 767, 'infected': 768, 'indeed': 769, 'incredible': 770, 'income': 771, 'including': 772, 'includes': 773, 'included': 774, 'inch': 775, 'incentive': 776, 'inane': 777, 'improved': 778, 'improperly': 779, 'imposing': 780, 'importantly': 781, 'implementation': 782, 'implement': 783, 'immigration': 784, 'imagine': 785, 'image': 786, 'illogical': 787, 'illegal': 788, 'ill': 789, 'ignores': 790, 'ignorant': 791, 'iets': 792, 'idgaf': 793, 'identity': 794, 'ideas': 795, 'id': 796, 'ice': 797, 'hype': 798, 'hvac': 799, 'husband': 800, 'humiliation': 801, 'huidskleur': 802, 'huffy': 803, 'hr': 804, 'house': 805, 'hours': 806, 'hot': 807, 'horrifying': 808, 'horribly': 809, 'hope': 810, 'home': 811, 'hit': 812, 'hijacking': 813, 'highest': 814, 'higher': 815, 'hierarchy': 816, 'hierarchies': 817, 'hey': 818, 'helps': 819, 'helped': 820, 'hell': 821, 'helicopter': 822, 'heinous': 823, 'heh': 824, 'heck': 825, 'heart': 826, 'heard': 827, 'healthy': 828, 'head': 829, 'hath': 830, 'hate': 831, 'harmed': 832, 'hardly': 833, 'happy': 834, 'happens': 835, 'happening': 836, 'happen': 837, 'hands': 838, 'handful': 839, 'hamstrung': 840, 'habit': 841, 'gurus': 842, 'guns': 843, 'gullible': 844, 'gulf': 845, 'guides': 846, 'guards': 847, 'guaranteed': 848, 'guarantee': 849, 'ground': 850, 'groep': 851, 'griefed': 852, 'gremist': 853, 'greatest': 854, 'grab': 855, 'gps': 856, 'governed': 857, 'gone': 858, 'giving': 859, 'gives': 860, 'girlfriend': 861, 'giggled': 862, 'gifts': 863, 'gesture': 864, 'genius': 865, 'genes': 866, 'gen': 867, 'geld': 868, 'gedyneith': 869, 'gediscrimineerd': 870, 'geaardheid': 871, 'gay': 872, 'gave': 873, 'gatling': 874, 'gather': 875, 'gate': 876, 'garden': 877, 'garbage': 878, 'gamer': 879, 'gaat': 880, 'fusion': 881, 'funny': 882, 'fuck': 883, 'frustrated': 884, 'fritz': 885, 'friend': 886, 'friday': 887, 'freya': 888, 'frequently': 889, 'foundation': 890, 'forward': 891, 'forms': 892, 'former': 893, 'forgotten': 894, 'football': 895, 'food': 896, 'focus': 897, 'floor': 898, 'flexibility': 899, 'flesh': 900, 'flawless': 901, 'flat': 902, 'finland': 903, 'fingers': 904, 'finals': 905, 'finally': 906, 'fill': 907, 'file': 908, 'fight': 909, 'fides': 910, 'felons': 911, 'feelings': 912, 'feedback': 913, 'fed': 914, 'feature': 915, 'fear': 916, 'fault': 917, 'father': 918, 'faster': 919, 'far': 920, 'fandom': 921, 'family': 922, 'faith': 923, 'fairy': 924, 'fairing': 925, 'failed': 926, 'fail': 927, 'faceoffs': 928, 'faceoff': 929, 'eyes': 930, 'extend': 931, 'exploring': 932, 'explore': 933, 'explains': 934, 'explaining': 935, 'explained': 936, 'experiment': 937, 'expenses': 938, 'expectations': 939, 'expect': 940, 'existent': 941, 'exceptions': 942, 'except': 943, 'example': 944, 'evinces': 945, 'evils': 946, 'evil': 947, 'evident': 948, 'evidence': 949, 'everywhere': 950, 'everything': 951, 'event': 952, 'european': 953, 'established': 954, 'establish': 955, 'espionage': 956, 'especially': 957, 'esoteric': 958, 'escapes': 959, 'error': 960, 'ergo': 961, 'erg': 962, 'equal': 963, 'episodes': 964, 'environment': 965, 'enorm': 966, 'enjoyed': 967, 'enige': 968, 'english': 969, 'engine': 970, 'enforcer': 971, 'enforceable': 972, 'enforce': 973, 'ends': 974, 'endowed': 975, 'ended': 976, 'encountered': 977, 'en': 978, 'empty': 979, 'emotionally': 980, 'eliminating': 981, 'elegant': 982, 'electric': 983, 'elector': 984, 'electing': 985, 'elect': 986, 'elder': 987, 'efficiently': 988, 'efficient': 989, 'effective': 990, 'effect': 991, 'edition': 992, 'edit': 993, 'ed': 994, 'easy': 995, 'easier': 996, 'earns': 997, 'earlier': 998, 'dude': 999, 'drowning': 1000, 'dropping': 1001, 'drop': 1002, 'driver': 1003, 'drinking': 1004, 'draws': 1005, 'drawn': 1006, 'dr': 1007, 'dq': 1008, 'doyalst': 1009, 'dota': 1010, 'doordaf': 1011, 'dominate': 1012, 'dolayst': 1013, 'dogs': 1014, 'dog': 1015, 'doesnt': 1016, 'dodge': 1017, 'docking': 1018, 'diy': 1019, 'disturbing': 1020, 'distinctive': 1021, 'dispute': 1022, 'disposed': 1023, 'disobey': 1024, 'dismiss': 1025, 'dishes': 1026, 'discussed': 1027, 'discourse': 1028, 'discount': 1029, 'disagree': 1030, 'director': 1031, 'dinovo': 1032, 'dining': 1033, 'difference': 1034, 'dictate': 1035, 'dialogue': 1036, 'diagram': 1037, 'devs': 1038, 'devices': 1039, 'develop': 1040, 'dev': 1041, 'detrimental': 1042, 'destructive': 1043, 'destroys': 1044, 'despotism': 1045, 'despite': 1046, 'despicable': 1047, 'designed': 1048, 'desi': 1049, 'deriving': 1050, 'dependence': 1051, 'democrats': 1052, 'delighted': 1053, 'definitely': 1054, 'defend': 1055, 'dedicated': 1056, 'declinegroupinvites': 1057, 'decisions': 1058, 'deciding': 1059, 'debate': 1060, 'deaths': 1061, 'death': 1062, 'deadlands': 1063, 'danger': 1064, 'dandy': 1065, 'dance': 1066, 'damn': 1067, 'dad': 1068, 'cuts': 1069, 'cut': 1070, 'custody': 1071, 'curled': 1072, 'cups': 1073, 'culturally': 1074, 'cubes': 1075, 'crowd': 1076, 'criticize': 1077, 'criticisms': 1078, 'creator': 1079, 'creates': 1080, 'created': 1081, 'create': 1082, 'cream': 1083, 'crap': 1084, 'craft': 1085, 'cover': 1086, 'course': 1087, 'couple': 1088, 'county': 1089, 'counterplay': 1090, 'count': 1091, 'cost': 1092, 'core': 1093, 'coordinated': 1094, 'cook': 1095, 'converting': 1096, 'conversion': 1097, 'conversation': 1098, 'contributions': 1099, 'continually': 1100, 'contain': 1101, 'consular': 1102, 'constitutionally': 1103, 'constantly': 1104, 'consistent': 1105, 'considering': 1106, 'consideration': 1107, 'consequence': 1108, 'consent': 1109, 'conquest': 1110, 'connection': 1111, 'connect': 1112, 'conditioning': 1113, 'condition': 1114, 'concerns': 1115, 'completion': 1116, 'complete': 1117, 'complain': 1118, 'competing': 1119, 'compensating': 1120, 'compared': 1121, 'compare': 1122, 'common': 1123, 'comes': 1124, 'combos': 1125, 'combat': 1126, 'colored': 1127, 'collectibles': 1128, 'collectible': 1129, 'collar': 1130, 'clue': 1131, 'clip': 1132, 'clinton': 1133, 'clearly': 1134, 'clear': 1135, 'classified': 1136, 'classes': 1137, 'claimed': 1138, 'claim': 1139, 'cl': 1140, 'citizens': 1141, 'cities': 1142, 'circumvented': 1143, 'chris': 1144, 'cheri': 1145, 'chem': 1146, 'checks': 1147, 'cheap': 1148, 'charlie': 1149, 'charges': 1150, 'characters': 1151, 'character': 1152, 'changing': 1153, 'champ': 1154, 'chairs': 1155, 'century': 1156, 'centuries': 1157, 'centra': 1158, 'censorship': 1159, 'cell': 1160, 'caution': 1161, 'causes': 1162, 'catch': 1163, 'cast': 1164, 'case': 1165, 'carolina': 1166, 'cards': 1167, 'card': 1168, 'car': 1169, 'capturing': 1170, 'capture': 1171, 'caps': 1172, 'candidates': 1173, 'canada': 1174, 'campaign': 1175, 'camp': 1176, 'came': 1177, 'bw': 1178, 'buying': 1179, 'buyer': 1180, 'busquets': 1181, 'bush': 1182, 'burst': 1183, 'burned': 1184, 'bunch': 1185, 'bums': 1186, 'bummer': 1187, 'bullshit': 1188, 'bullets': 1189, 'bulk': 1190, 'build': 1191, 'bug': 1192, 'budget': 1193, 'buddy': 1194, 'buddhist': 1195, 'bucks': 1196, 'bubzbeauty': 1197, 'brown': 1198, 'broken': 1199, 'british': 1200, 'bring': 1201, 'brawler': 1202, 'branches': 1203, 'branch': 1204, 'boys': 1205, 'boring': 1206, 'boosted': 1207, 'bonuses': 1208, 'bones': 1209, 'bona': 1210, 'bomb': 1211, 'body': 1212, 'bodies': 1213, 'boat': 1214, 'board': 1215, 'boa': 1216, 'blue': 1217, 'block': 1218, 'blackbough': 1219, 'biz': 1220, 'binnen': 1221, 'bill': 1222, 'bike': 1223, 'bigot': 1224, 'bigger': 1225, 'bigbird': 1226, 'between': 1227, 'betting': 1228, 'betekent': 1229, 'bet': 1230, 'besides': 1231, 'benefits': 1232, 'beneficiaries': 1233, 'belonging': 1234, 'bellies': 1235, 'beledigd': 1236, 'behind': 1237, 'beauty': 1238, 'beautiful': 1239, 'beaten': 1240, 'bc': 1241, 'bb': 1242, 'batoto': 1243, 'bathing': 1244, 'basis': 1245, 'bases': 1246, 'barraging': 1247, 'banning': 1248, 'banksville': 1249, 'bandwagon': 1250, 'ban': 1251, 'baltimore': 1252, 'balances': 1253, 'balance': 1254, 'bacteria': 1255, 'baby': 1256, 'azimuth': 1257, 'awkward': 1258, 'awesome': 1259, 'average': 1260, 'ave': 1261, 'author': 1262, 'australian': 1263, 'august': 1264, 'attention': 1265, 'attempt': 1266, 'attacking': 1267, 'assumptions': 1268, 'assignments': 1269, 'asshole': 1270, 'assembly': 1271, 'asleep': 1272, 'asked': 1273, 'artists': 1274, 'article': 1275, 'army': 1276, 'armor': 1277, 'area': 1278, 'approaching': 1279, 'appreciates': 1280, 'appraiser': 1281, 'appellate': 1282, 'apparently': 1283, 'apologize': 1284, 'antithetic': 1285, 'answers': 1286, 'annoyed': 1287, 'angry': 1288, 'although': 1289, 'alter': 1290, 'alone': 1291, 'almost': 1292, 'allows': 1293, 'allergic': 1294, 'alert': 1295, 'ale': 1296, 'alcohol': 1297, 'air': 1298, 'ahead': 1299, 'agreed': 1300, 'against': 1301, 'afterburner': 1302, 'afkomst': 1303, 'affordable': 1304, 'afford': 1305, 'afdoen': 1306, 'advocates': 1307, 'advisor': 1308, 'advertised': 1309, 'adopting': 1310, 'admittedly': 1311, 'admired': 1312, 'admin': 1313, 'adds': 1314, 'address': 1315, 'added': 1316, 'add': 1317, 'actual': 1318, 'activist': 1319, 'actions': 1320, 'acquisition': 1321, 'acknowledgements': 1322, 'accustomed': 1323, 'accuse': 1324, 'accountable': 1325, 'account': 1326, 'accordingly': 1327, 'accidents': 1328, 'accessory': 1329, 'acceptable': 1330, 'abuses': 1331, 'abused': 1332, 'absolutely': 1333, 'absolute': 1334, 'absentee': 1335, 'abortion': 1336, 'abolishing': 1337, 'abolish': 1338, 'aan': 1339, '99': 1340, '82': 1341, '75': 1342, '60mph': 1343, '60': 1344, '4th': 1345, '4': 1346, '388': 1347, '354194728': 1348, '2nd': 1349, '2c': 1350, '2015': 1351, '20': 1352, '1x1x1': 1353, '14': 1354, '13': 1355, '12v': 1356, '1': 1357, '000': 1358, '0': 1359, \"'yyy\": 1360, \"'xxx\": 1361, \"'vapor\": 1362, \"'looking\": 1363, 'zich': 1364, 'youre': 1365, 'younger': 1366, 'x': 1367, 'wrote': 1368, 'written': 1369, 'worst': 1370, 'worlds': 1371, 'works': 1372, 'working': 1373, 'word': 1374, 'wonka': 1375, 'women': 1376, 'withdrawal': 1377, 'williams': 1378, 'wij': 1379, 'whether': 1380, 'whenever': 1381, 'watsonian': 1382, 'voelen': 1383, 'vice': 1384, 'versions': 1385, 'version': 1386, 'var': 1387, 'vaccine': 1388, 'v': 1389, 'utah': 1390, 'using': 1391, 'usc': 1392, 'us': 1393, 'understand': 1394, 'trying': 1395, 'try': 1396, 'townsville': 1397, 'toronto': 1398, 'thrown': 1399, 'thread': 1400, 'thought': 1401, 'third': 1402, 'technically': 1403, 'taxes': 1404, 'tax': 1405, 'talking': 1406, 'talk': 1407, 'taking': 1408, 'sydney': 1409, 'sweet': 1410, 'surprised': 1411, 'support': 1412, 'super': 1413, 'suffer': 1414, 'successfully': 1415, 'strong': 1416, 'striker': 1417, 'storage': 1418, 'stephen': 1419, 'stars': 1420, 'standard': 1421, 'squad': 1422, 'spoiler': 1423, 'special': 1424, 'sorry': 1425, 'song': 1426, 'son': 1427, 'sometimes': 1428, 'solar': 1429, 'smite': 1430, 'slight': 1431, 'skill': 1432, 'sitting': 1433, 'single': 1434, 'simply': 1435, 'similar': 1436, 'side': 1437, 'ships': 1438, 'ship': 1439, 'service': 1440, 'sentence': 1441, 'sellers': 1442, 'seller': 1443, 'section': 1444, 'scratch': 1445, 'scope': 1446, 'scared': 1447, 'scam': 1448, 'save': 1449, 'rush': 1450, 'roles': 1451, 'riedewald': 1452, 'ridiculous': 1453, 'rewarded': 1454, 'reward': 1455, 'remove': 1456, 'relevant': 1457, 'releases': 1458, 'regularly': 1459, 'reduce': 1460, 'reciprocated': 1461, 'recently': 1462, 'reason': 1463, 'real': 1464, 'rare': 1465, 'ranked': 1466, 'radius': 1467, 'r': 1468, 'quite': 1469, 'questions': 1470, 'quality': 1471, 'qb': 1472, 'putting': 1473, 'pushed': 1474, 'professional': 1475, 'private': 1476, 'prison': 1477, 'pride': 1478, 'preservatives': 1479, 'practice': 1480, 'ppr': 1481, 'powers': 1482, 'power': 1483, 'possible': 1484, 'playstyle': 1485, 'plan': 1486, 'physics': 1487, 'physical': 1488, 'personally': 1489, 'person': 1490, 'period': 1491, 'past': 1492, 'part': 1493, 'paper': 1494, 'paid': 1495, 'outside': 1496, 'opinion': 1497, 'ons': 1498, 'olp': 1499, 'ok': 1500, 'oh': 1501, 'off': 1502, 'normally': 1503, 'nobody': 1504, 'nigel': 1505, 'needs': 1506, 'name': 1507, 'myself': 1508, 'multiple': 1509, 'money': 1510, 'mirrors': 1511, 'midfielder': 1512, 'mensen': 1513, 'men': 1514, 'means': 1515, 'match': 1516, 'masonry': 1517, 'mankind': 1518, 'makes': 1519, 'luck': 1520, 'looks': 1521, 'looking': 1522, 'long': 1523, 'literally': 1524, 'listen': 1525, 'list': 1526, 'light': 1527, 'license': 1528, 'left': 1529, 'learn': 1530, 'league': 1531, 'lead': 1532, 'lb': 1533, 'late': 1534, 'knife': 1535, 'kill': 1536, 'kick': 1537, 'jong': 1538, 'job': 1539, 'je': 1540, 'issues': 1541, 'issue': 1542, 'involving': 1543, 'invited': 1544, 'internet': 1545, 'influenced': 1546, 'imo': 1547, 'immediately': 1548, 'however': 1549, 'honest': 1550, 'holds': 1551, 'hitting': 1552, 'history': 1553, 'himself': 1554, 'het': 1555, 'hero': 1556, 'hard': 1557, 'happiness': 1558, 'guess': 1559, 'green': 1560, 'grappler': 1561, 'granted': 1562, 'governments': 1563, 'gods': 1564, 'goddamn': 1565, 'god': 1566, 'gl': 1567, 'giveaway': 1568, 'getting': 1569, 'gets': 1570, 'gently': 1571, 'gem': 1572, 'gear': 1573, 'gbr': 1574, 'future': 1575, 'front': 1576, 'free': 1577, 'fly': 1578, 'flags': 1579, 'fit': 1580, 'films': 1581, 'film': 1582, 'files': 1583, 'fewer': 1584, 'felt': 1585, 'fashion': 1586, 'fanart': 1587, 'fan': 1588, 'fall': 1589, 'faggot': 1590, 'fact': 1591, 'expierenced': 1592, 'experience': 1593, 'existing': 1594, 'exclusive': 1595, 'europe': 1596, 'etc': 1597, 'ernstig': 1598, 'eredivisie': 1599, 'equations': 1600, 'enter': 1601, 'energy': 1602, 'enemy': 1603, 'educational': 1604, 'early': 1605, 'eap': 1606, 'duty': 1607, 'dutch': 1608, 'dunno': 1609, 'dumb': 1610, 'drug': 1611, 'draw': 1612, 'document': 1613, 'doctors': 1614, 'dishonest': 1615, 'discussions': 1616, 'discussion': 1617, 'difficult': 1618, 'design': 1619, 'deserves': 1620, 'deserve': 1621, 'defense': 1622, 'decks': 1623, 'deck': 1624, 'decide': 1625, 'decent': 1626, 'decades': 1627, 'currently': 1628, 'current': 1629, 'curiosity': 1630, 'cube': 1631, 'cool': 1632, 'convince': 1633, 'content': 1634, 'considered': 1635, 'concerned': 1636, 'concept': 1637, 'collection': 1638, 'children': 1639, 'child': 1640, 'changes': 1641, 'changed': 1642, 'chance': 1643, 'certain': 1644, 'centered': 1645, 'casual': 1646, 'cars': 1647, 'capital': 1648, 'capacitors': 1649, 'cairns': 1650, 'building': 1651, 'bought': 1652, 'blowing': 1653, 'blind': 1654, 'bit': 1655, 'billions': 1656, 'bikes': 1657, 'beyond': 1658, 'benefit': 1659, 'beneficiary': 1660, 'believe': 1661, 'becomes': 1662, 'become': 1663, 'bay': 1664, 'based': 1665, 'base': 1666, 'bar': 1667, 'balanced': 1668, 'badly': 1669, 'backers': 1670, 'avoid': 1671, 'available': 1672, 'attacks': 1673, 'attack': 1674, 'ass': 1675, 'arsenal': 1676, 'armored': 1677, 'argument': 1678, 'application': 1679, 'appeal': 1680, 'anyway': 1681, 'anxiety': 1682, 'anti': 1683, 'annoying': 1684, 'anita': 1685, 'among': 1686, 'along': 1687, 'allowed': 1688, 'alchemist': 1689, 'ago': 1690, 'af': 1691, 'aetherial': 1692, 'adult': 1693, 'adjusted': 1694, 'ad': 1695, 'across': 1696, 'accumulators': 1697, 'ability': 1698, '9': 1699, '40': 1700, '1960': 1701, '00': 1702, 'year': 1703, 'yeah': 1704, 'went': 1705, 'ways': 1706, 'wants': 1707, 'vn': 1708, 'vapor': 1709, 'van': 1710, 'until': 1711, 'type': 1712, 'tweet': 1713, 'turn': 1714, 'told': 1715, 'today': 1716, 'times': 1717, 'thrusters': 1718, 'three': 1719, 'thank': 1720, 'tete': 1721, 'test': 1722, 'teacher': 1723, 'surface': 1724, 'sure': 1725, 'stop': 1726, 'started': 1727, 'stance': 1728, 'spend': 1729, 'specific': 1730, 'social': 1731, 'small': 1732, 'skills': 1733, 'silent': 1734, 'shift': 1735, 'shane': 1736, 'sell': 1737, 'seem': 1738, 'season': 1739, 'scientific': 1740, 'school': 1741, 'roughly': 1742, 'road': 1743, 'remember': 1744, 'read': 1745, 'probleem': 1746, 'president': 1747, 'please': 1748, 'player': 1749, 'pl': 1750, 'paste': 1751, 'parents': 1752, 'panthers': 1753, 'pages': 1754, 'output': 1755, 'others': 1756, 'original': 1757, 'option': 1758, 'ones': 1759, 'ondp': 1760, 'old': 1761, 'nothing': 1762, 'nen': 1763, 'nearly': 1764, 'na': 1765, 'mods': 1766, 'mind': 1767, 'mandatory': 1768, 'low': 1769, 'logic': 1770, 'little': 1771, 'likely': 1772, 'life': 1773, 'level': 1774, 'lessons': 1775, 'large': 1776, 'kind': 1777, 'kids': 1778, 'keep': 1779, 'judge': 1780, 'interesting': 1781, 'information': 1782, 'include': 1783, 'important': 1784, 'im': 1785, 'idea': 1786, 'huge': 1787, 'honestly': 1788, 'hold': 1789, 'high': 1790, 'hand': 1791, 'google': 1792, 'gon': 1793, 'go': 1794, 'give': 1795, 'games': 1796, 'furniture': 1797, 'form': 1798, 'flying': 1799, 'find': 1800, 'figure': 1801, 'felon': 1802, 'feel': 1803, 'extremely': 1804, 'excellent': 1805, 'excel': 1806, 'exactly': 1807, 'events': 1808, 'entire': 1809, 'driving': 1810, 'done': 1811, 'doing': 1812, 'dit': 1813, 'deal': 1814, 'days': 1815, 'date': 1816, 'dan': 1817, 'convict': 1818, 'conservative': 1819, 'comfortable': 1820, 'close': 1821, 'city': 1822, 'checked': 1823, 'charge': 1824, 'bucs': 1825, 'brock': 1826, 'big': 1827, 'batteries': 1828, 'back': 1829, 'away': 1830, 'answer': 1831, 'another': 1832, 'allow': 1833, 'agree': 1834, 'additional': 1835, 'able': 1836, 'ab': 1837, '8': 1838, '7': 1839, '5000': 1840, '5': 1841, '300': 1842, '30': 1843, 'zen': 1844, 'yet': 1845, 'where': 1846, 'week': 1847, 'vote': 1848, 'true': 1849, 'tier': 1850, 'tell': 1851, 'start': 1852, 'since': 1853, 'seems': 1854, 'safe': 1855, 's': 1856, 'rights': 1857, 'require': 1858, 'provide': 1859, 'playing': 1860, 'played': 1861, 'per': 1862, 'niet': 1863, 'need': 1864, 'near': 1865, 'months': 1866, 'miles': 1867, 'may': 1868, 'making': 1869, 'lose': 1870, 'live': 1871, 'less': 1872, 'items': 1873, 'instead': 1874, 'hour': 1875, 'him': 1876, 'got': 1877, 'everyone': 1878, 'else': 1879, 'een': 1880, 'each': 1881, 'during': 1882, 'dream': 1883, 'down': 1884, 'dont': 1885, 'damage': 1886, 'control': 1887, 'community': 1888, 'come': 1889, 'client': 1890, 'ca': 1891, 'both': 1892, 'battery': 1893, 'background': 1894, 'asking': 1895, 'around': 1896, 'amount': 1897, '3': 1898, '15': 1899, '100': 1900, 'worth': 1901, 'work': 1902, 'win': 1903, 'while': 1904, 'vanilla': 1905, 'vaccines': 1906, 'used': 1907, 'under': 1908, 'through': 1909, 'though': 1910, 'system': 1911, 'sound': 1912, 'someone': 1913, 'shit': 1914, 'run': 1915, 'rules': 1916, 'risk': 1917, 'reading': 1918, 'pretty': 1919, 'over': 1920, 'new': 1921, 'maybe': 1922, 'look': 1923, 'half': 1924, 'fun': 1925, 'fucking': 1926, 'friends': 1927, 'found': 1928, 'fine': 1929, 'f': 1930, 'end': 1931, 'different': 1932, 'die': 1933, 'dat': 1934, 'consider': 1935, 'chapter': 1936, 'basically': 1937, 'ask': 1938, 'am': 1939, 'always': 1940, 'als': 1941, 'already': 1942, 'again': 1943, '6': 1944, 'years': 1945, 'without': 1946, 'were': 1947, 'want': 1948, 'vs': 1949, 'time': 1950, 'thing': 1951, 'teams': 1952, 'still': 1953, 'something': 1954, 'second': 1955, 'said': 1956, 'points': 1957, 'panels': 1958, 'now': 1959, 'non': 1960, 'many': 1961, 'made': 1962, 'least': 1963, 'help': 1964, 'few': 1965, 'enough': 1966, 'either': 1967, 'de': 1968, 'completely': 1969, 'bombs': 1970, 'best': 1971, 'anyone': 1972, 'after': 1973, 'actually': 1974, \"'ve\": 1975, \"'d\": 1976, \"'\": 1977, 'yourself': 1978, 'top': 1979, 'such': 1980, 'stuff': 1981, 'she': 1982, 'put': 1983, 'probably': 1984, 'point': 1985, 'own': 1986, 'our': 1987, 'op': 1988, 'never': 1989, 'lot': 1990, 'last': 1991, 'its': 1992, 'having': 1993, 'gun': 1994, 'government': 1995, 'every': 1996, 'check': 1997, 'bad': 1998, 'anything': 1999, '2': 2000, \"'ll\": 2001, 'very': 2002, 'those': 2003, 'these': 2004, 'team': 2005, 'take': 2006, 'players': 2007, 'play': 2008, 'day': 2009, 'wo': 2010, 'way': 2011, 'mod': 2012, 'how': 2013, 'here': 2014, 'great': 2015, 'first': 2016, 'could': 2017, 'better': 2018, 'being': 2019, 'see': 2020, 'into': 2021, 'too': 2022, 'right': 2023, 'place': 2024, 'her': 2025, 'before': 2026, 'well': 2027, 'use': 2028, 'then': 2029, 'same': 2030, 'mean': 2031, 'make': 2032, 'good': 2033, 'by': 2034, 'also': 2035, \"'m\": 2036, 'say': 2037, 'only': 2038, 'going': 2039, 'which': 2040, 'really': 2041, 'much': 2042, 'most': 2043, 'know': 2044, 'from': 2045, 'did': 2046, '!': 2047, 'should': 2048, 'other': 2049, 'his': 2050, 'gt': 2051, 'why': 2052, 'we': 2053, 'up': 2054, 'one': 2055, 'had': 2056, 'game': 2057, 'even': 2058, 'does': 2059, 'because': 2060, 'will': 2061, 'than': 2062, \"'re\": 2063, 'more': 2064, 'think': 2065, 'been': 2066, 'any': 2067, 'some': 2068, 'me': 2069, 'my': 2070, 'no': 2071, 'can': 2072, 'an': 2073, 'about': 2074, '?': 2075, 'who': 2076, 'would': 2077, 'your': 2078, 'when': 2079, 'so': 2080, 'get': 2081, '(': 2082, 'their': 2083, 'out': 2084, 'has': 2085, 'all': 2086, 'them': 2087, 'people': 2088, ')': 2089, 'what': 2090, 'there': 2091, 'he': 2092, 'at': 2093, 'just': 2094, 'this': 2095, 'if': 2096, 'or': 2097, 'was': 2098, 'with': 2099, 'on': 2100, 'like': 2101, 'not': 2102, 'but': 2103, 'are': 2104, 'be': 2105, 'they': 2106, 'as': 2107, 'have': 2108, 'do': 2109, 'for': 2110, '\\\\': 2111, \"'s\": 2112, 'in': 2113, 'is': 2114, \"n't\": 2115, 'that': 2116, 'SENTENCE_START': 2117, 'SENTENCE_END': 2118, 'of': 2119, 'it': 2120, 'you': 2121, 'i': 2122, 'a': 2123, 'and': 2124, 'to': 2125, ',': 2126, 'the': 2127}\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 8000\n",
    "\n",
    "def buildVocab(sentences):\n",
    "    # Tokenize the sentences into words\n",
    "    tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "    ####### If we want to order the words in alphabet #######\n",
    "    # Count the word frequencies\n",
    "    word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "    print(\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
    "\n",
    "    # Get the most common words and build index_to_word and word_to_index vectors\n",
    "    vocab = sorted(word_freq.items(), key=lambda x: (x[1], x[0]), reverse=True)[:vocabulary_size-2]\n",
    "    print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "    print(\"The most frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[0][0], vocab[0][1]))\n",
    "    print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
    " \n",
    "    sorted_vocab = sorted(vocab, key=operator.itemgetter(1))\n",
    "    index_to_word = [\"<MASK/>\", unknown_token] + [x[0] for x in sorted_vocab]\n",
    "    word_to_index = dict([(w, i) for i, w in enumerate(index_to_word)])\n",
    "    \n",
    "    \n",
    "    print(word_to_index)\n",
    "    return index_to_word, word_to_index\n",
    "    \n",
    "index_to_word, word_to_index = buildVocab(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2117, 2116, 2059, 2115, 1633, 2121, 1, 2118] SENTENCE_START that does n't convince you \\? SENTENCE_END\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "####### In case use all sentences of each element in the df.body ######\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "tokenized_sentences = []\n",
    "for i, sent in enumerate(sents):\n",
    "    temp = [word_to_index[w] if w in word_to_index else word_to_index[unknown_token] for w in sent.split()]\n",
    "    tokenized_sentences.append(temp)\n",
    "\n",
    "# #convert text to integers\n",
    "# tokenized_sents = [[word_to_index[c] if c in word_to_index else word_to_index[unknown_token] \\\n",
    "#                                        for c in s] for s in sents.split()]\n",
    "print(tokenized_sentences[3], sents[3])\n",
    "X_train_word = tokenized_sentences\n",
    "len(X_train_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxlen(sentence)= 463\n",
      "0    [2117, 2122, 742, 2123, 1921, 1531, 2095, 1703...\n",
      "1    [2117, 2113, 2078, 336, 2126, 2123, 1490, 2017...\n",
      "2    [2117, 2106, 2109, 2115, 2081, 1495, 2110, 201...\n",
      "3    [2117, 2122, 1609, 2126, 1829, 2026, 2127, 126...\n",
      "4    [2117, 2071, 2126, 2103, 154, 1428, 2077, 1612...\n",
      "Name: body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert training data's text to integer\n",
    "\n",
    "####### In case considering each row in df.body is a sentence ######\n",
    "\n",
    "lenword_of_row = X_train.str.split().str.len().max() \n",
    "\n",
    "print(\"maxlen(sentence)=\",lenword_of_row)\n",
    "\n",
    "def manipulateSentence(s, sequence_length, min_sent_characters=1):\n",
    "    if (\"http\" not in s and len(s) >= min_sent_characters):\n",
    "         s = clean_str(s)\n",
    "    \n",
    "    #Pad special words. Note: Adding <START/END> is not obligation here as 1 row is 1 sentence\n",
    "    num_padding = sequence_length - len(s.split())\n",
    "    if num_padding >0:\n",
    "        temp = str([unknown_token] * num_padding)\n",
    "        s = ' '.join([s, temp])\n",
    "\n",
    "    # Append SENTENCE_START and SENTENCE_END\n",
    "    s = \" \".join([sentence_start_token, s, sentence_end_token])\n",
    "    \n",
    "    #convert word to integer\n",
    "    s = [word_to_index[c] if c in word_to_index else word_to_index[unknown_token] \\\n",
    "                                for c in s.split()]\n",
    "    if (len(s)!=sequence_length+2): print(len(s), sequence_length)\n",
    "    return s\n",
    "\n",
    "ss = X_train.map(lambda s: manipulateSentence(s, lenword_of_row))\n",
    "print(ss.head())\n",
    "\n",
    "lenword_of_row+=2\n",
    "# ss = X_train.map(lambda x: [word_to_index[c] if c in word_to_index else word_to_index[unknown_token] \\\n",
    "#                                        for c in x.split()] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 465)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_temp = ss.values #[[5,2],[1,2,3],[1]]\n",
    "length = len(sorted(x_temp,key=len, reverse=True)[0])\n",
    "X_train_word = np.array([xi+[None]*(length-len(xi)) for xi in x_temp])\n",
    "X_train_word.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char-level Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 200 # nb of neurons in hidden layer\n",
    "seq_length = 25 # number of steps to unroll the RNN for, and this is also nb of chars putting in the input of RNN\n",
    "learning_rate = 1e-1 # learning rate of training\n",
    "\n",
    "vocab_size = len(chars) + 1 #M\n",
    "wordvec_size = vocab_size #D\n",
    "\n",
    "#init parameters\n",
    "W_hh = np.random.randn(hidden_size, hidden_size) #(H,H)\n",
    "W_xh = np.random.randn(wordvec_size, hidden_size) #(D,H)\n",
    "W_hy = np.random.randn(hidden_size, vocab_size) #(H,M), M=D for mode='char'\n",
    "b_h = np.zeros((1,hidden_size)) #(H,)\n",
    "b_y = np.zeros((1,vocab_size)) #(M,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference from @Karpathy: https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "#\n",
    "#                         [b_h]                                                [b_y]\n",
    "#    w2v                    v                                (h_next)            v\n",
    "#  x --> x_s -> [W_xh] -> [sum] -> h_raw -> [nonlinearity] -> h_s -> [W_hy] -> [sum] -> y_s -> [exp(y[k])/sum(exp(y))] -> p_s\n",
    "#                           ^                                  |\n",
    "#                           '----h_prev------[W_hh]------------'\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward_single_step(x, h_prev, W_xh, W_hh, W_hy, b_h, b_y):\n",
    "    \"\"\"\n",
    "    Run the forward pass for a single timestep of a vanilla RNN that uses a tanh\n",
    "    activation function.\n",
    "\n",
    "    The input data has dimension D, the hidden state has dimension H, and we use\n",
    "    a minibatch size of N.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data for this timestep, of shape (N, D).\n",
    "    - h_prev: Hidden state from previous timestep, of shape (N, H)\n",
    "    - W_xh: Weight matrix for input-to-hidden connections, of shape (D, H)\n",
    "    - W_hh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n",
    "    - W_hy: Weight matrix for hidden-to-output connections, of shape (H, M)\n",
    "    - b_h: Biases of shape (H,)\n",
    "    - b_y: Bias of shape (M, )\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - h_next: Next hidden state, of shape (N, H)\n",
    "    - y_s: output of this timestep (N, M)\n",
    "    - cache: Tuple of values needed for the backward pass.\n",
    "    \"\"\"\n",
    "    \n",
    "    h_raw = np.dot(x, W_xh) + np.dot(h_prev, W_hh) + b_h  #(N,D)x(D,H) + (N,H)x(H,H) +(1,H) = (N,H)\n",
    "    h_next = np.tanh(h_raw) # hidden nodes, (N, H)\n",
    "    y_s = np.dot(h_next, W_hy) + b_y #output, (N, H)x(H,M) +(1,M) =(N,M)\n",
    "#     p_s = np.exp(y_s) / np.sum(np.exp(y_s), axis=0) #softmax\n",
    "    \n",
    "    cache = (x, h_prev, h_next, W_xh, W_hh, W_hy, b_h, b_y)\n",
    "    return h_next, y_s, cache\n",
    "\n",
    "\n",
    "def rnn_forward(x, h0, W_xh, W_hh, W_hy, b_h, b_y):\n",
    "    \"\"\"\n",
    "    Run a vanilla RNN forward on an entire sequence of data. We assume an input\n",
    "    sequence composed of T vectors (each vector represents a word/char), each of dimension D. \n",
    "    The RNN uses a hidden\n",
    "    size of H, and we work over a minibatch containing N sequences. After running\n",
    "    the RNN forward, we return the hidden states for all timesteps.\n",
    "    Inputs:\n",
    "    - x: Input data for the entire timeseries, of shape (N, T, D).\n",
    "    - h0: Initial hidden state, of shape (N, H)\n",
    "    - W_xh: Weight matrix for input-to-hidden connections, of shape (D, H)\n",
    "    - W_hh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n",
    "    - W_hy: Weight matrix for hidden-to-output connections, of shape (H, M)\n",
    "    - b_h: Biases of shape (H,)\n",
    "    - b_y: Bias of shape (M, )\n",
    "    Returns a tuple of:\n",
    "    - h: Hidden states for the entire timeseries, of shape (N, T, H)\n",
    "    - y: Output of the entire timeseries, of shape (N, T, M)\n",
    "    - cache: Values needed in the backward pass\n",
    "    \"\"\"\n",
    "    N, T, D = x.shape\n",
    "    H, M = W_hy.shape\n",
    "    \n",
    "    h = np.empty((N, T, H))\n",
    "    cache = {}\n",
    "    y_s = np.empty((N, T, M))\n",
    "    \n",
    "#     print(\"N=%d, T=%d, D=%d, H=%d, M=%d\" %(N, T,D,H,M))\n",
    "    for i in range(T):\n",
    "        if i==0: \n",
    "            h[:, i, :], y_s[:, i, :], cache[i] = rnn_forward_single_step(x[:,i,:], h0, W_xh, W_hh, W_hy, b_h, b_y)\n",
    "        else: \n",
    "            h[:, i, :], y_s[:, i, :], cache[i] = rnn_forward_single_step(x[:,i,:], h[:, i-1, :], W_xh, W_hh, W_hy, b_h, b_y)\n",
    "    \n",
    "    return h, y_s, cache\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference from @Karpathy: https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "#\n",
    "#                         [b_h] (1,H)                                         [b_y] (1,M)\n",
    "#    w2v                    v      (N,H)                     (N,H)   (H,M)       v\n",
    "#  x --> x_s -> [W_xh] -> [sum] -> h_raw -> [nonlinearity] -> h_s -> [W_hy] -> [sum] -> y_s -> [exp(y[k])/sum(exp(y))] -> p_s\n",
    "# (N,D)         (D,H)       ^                                  |                       (N,M)\n",
    "#                           '----h_prev------[W_hh]------------'\n",
    "#                                (N,H)       (H,H)\n",
    "\n",
    "def rnn_backward_single_step (dh_next, dy, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for a single timestep of a vanilla RNN.\n",
    "    Inputs:\n",
    "    - dh_next: Gradient of loss with respect to next hidden state (N, H)\n",
    "    - dy: of shape (N, M)\n",
    "    - cache: Cache object from the forward pass\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradients of input data, of shape (N, D)\n",
    "    - dh_prev: Gradients of previous hidden state, of shape (N, H)\n",
    "    - dWxh: Gradients of input-to-hidden weights, of shape (D, H)\n",
    "    - dWhh: Gradients of hidden-to-hidden weights, of shape (H, H)\n",
    "    - dWhy: Gradients of hidden-to-output weights, of shape (H, M)\n",
    "    - dbh: Gradients of bias vector, of shape (H,)\n",
    "    - dby: Gradients of bias vector, of shape (M,)\n",
    "    \n",
    "    \"\"\"\n",
    "    dx, dh_prev, dWxh, dWhh, dWhy, dbh, dby = None, None, None, None, None, None, None\n",
    "    x, h_prev, h_next, W_xh, W_hh, W_hy, b_h, b_y = cache\n",
    "    \n",
    "    dby = np.sum(dy, axis=0) # backprop of softmax, shape=(1,M)\n",
    "    dWhy = np.dot(h_next.T, dy) #(H,N)x(N,M) = (H,M)\n",
    "    dh = np.dot(dy, W_hy.T) + dh_next # backprop into h, (N,M)x(M,H)=(N,H)\n",
    "\n",
    "    dh_raw = (1 - h_next ** 2) * dh # note: tanh(x)' = 1 - tanh^2(x), shape=(N,H)\n",
    "    dbh = np.sum(dh_raw, axis=0) #(1,H)\n",
    "    dWxh = np.dot(x.T, dh_raw) # (D,N)x(N,H) = (D,H)\n",
    "    dWhh = np.dot(h_prev.T, dh_raw) # (H,N)x(N,H)=(H,H)\n",
    "    dx = np.dot(dh_raw, W_xh.T) # (N,H)x(H,D) = (N,D)\n",
    "    dh_prev = np.dot(dh_raw, W_hh.T) # (N,H)x(H,H) = (N,H)\n",
    "\n",
    "    ########\n",
    "#     dpre_actv = (1 - next_h ** 2) * dnext_h         # (N, H)\n",
    "#     dx = dpre_actv.dot(Wx.T)\n",
    "#     dprev_h = dpre_actv.dot(Wh.T) #(N,H)x(H,H)=(N,H)\n",
    "#     dWx = x.T.dot(dpre_actv)\n",
    "#     dWh = prev_h.T.dot(dpre_actv)\n",
    "#     db = np.sum(dpre_actv, 0)\n",
    "    \n",
    "    \n",
    "#     dWhy += np.dot(dy, hs[t].T)\n",
    "#     dby += dy\n",
    "#     dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "#     dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "#     dbh += dhraw\n",
    "#     dWxh += np.dot(dhraw, xs[t].T)\n",
    "#     dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "#     dhnext = np.dot(Whh.T, dhraw)\n",
    "\n",
    "    return dx, dh_prev, dWxh, dWhh, dWhy, dbh, dby\n",
    "\n",
    "\n",
    "def rnn_backward(dy, cache):\n",
    "    \"\"\"\n",
    "    Compute the backward pass for a vanilla RNN over an entire sequence of data.\n",
    "    Inputs:\n",
    "    - dy: Upstream gradients of softmax output, of shape (N, T, M)\n",
    "    - cache: cache objects from softmax output  \n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient of inputs, of shape (N, T, D)\n",
    "    - dh0: Gradient of initial hidden state, of shape (N, H)\n",
    "    - dWx: Gradient of input-to-hidden weights, of shape (D, H)\n",
    "    - dWh: Gradient of hidden-to-hidden weights, of shape (H, H)\n",
    "    - dbh: Gradient of biases, of shape (H,)\n",
    "    - dby: Gradient of biases, of shape (M,)\n",
    "    \"\"\"\n",
    "    \n",
    "   ## Calculate the upstreams gradients of all hidden states (dh) from softmax results\n",
    "    #i.e, temporal affine softmax backward\n",
    "#     Wx, Wh, b, x, prev_h, next_h = cache[0]\n",
    "#     N, T, H = dh.shape\n",
    "#     D, H = Wx.shape\n",
    "\n",
    "#     # Initialise gradients.\n",
    "#     dx = np.zeros([N, T, D])\n",
    "#     dWx = np.zeros_like(Wx)\n",
    "#     dWh = np.zeros_like(Wh)\n",
    "#     db = np.zeros_like(b)\n",
    "# dprev_h = np.zeros_like(prev_h)\n",
    "\n",
    "\n",
    "    N, T, M = dy.shape\n",
    "    D, H = wordvec_size, hidden_size\n",
    "\n",
    "    \n",
    "    dx = np.empty((N,T,D))\n",
    "    dWxh = np.zeros((D,H))\n",
    "    dWhh = np.zeros((H,H))\n",
    "    dWhy = np.zeros((H,M))\n",
    "    dbh = np.zeros((1,H))\n",
    "    dby = np.zeros((1,M))    \n",
    "    \n",
    "    dh_next = np.zeros((N, H))\n",
    "    \n",
    "    for i in reversed(range(T)):\n",
    "#         print(\"i=\",i, \"dh_next=\", dh_next.shape, \"dy=\", dy[:,i,:].shape)\n",
    "        dx[:,i,:], dh_next, dWxh_temp, dWhh_temp, dWhy_temp, dbh_temp, dby_temp = \\\n",
    "                rnn_backward_single_step(dh_next, dy[:,i,:], cache[i])\n",
    "        \n",
    "        dWxh += dWxh_temp\n",
    "        dWhy += dWhy_temp\n",
    "        dWhh += dWhh_temp\n",
    "        dbh += dbh_temp\n",
    "        dby += dby_temp\n",
    "    \n",
    "    dh0 = dh_next\n",
    "    return dx, dh0, dWxh, dWhh, dWhy, dbh, dby\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine forward/backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Affine forward\n",
    "def affine_forward(x, W, b):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    x - Input data, of shape (N, D) #may be generally (N,d1,...d_k) \n",
    "    w - Weights, of shape (D, M)\n",
    "    b - Biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    \n",
    "    out = np.dot(np.reshape(x,(N,-1)), W) + b #out = xW+b\n",
    "    cache = (x, W, b)\n",
    "    return out, cache\n",
    "\n",
    "## Affine backward\n",
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "      - x: Input data, of shape (N, D)\n",
    "      - w: Weights, of shape (D, M)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, D)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"    \n",
    "    x, W, b = cache\n",
    "    db = np.sum(dout, axis=0)\n",
    "    dx = dout.dot(W.T).reshape(x.shape)\n",
    "    dw = x.reshape(x.shape[0], -1).T.dot(dout)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_softmax_loss (y_s, y, mask, verbose=False):\n",
    "    \"\"\"\n",
    "    A temporal version of softmax loss for use in RNNs. We assume that we are\n",
    "    making predictions over a vocabulary of size M for each timestep of a\n",
    "    timeseries of length T, over a minibatch of size N. The input y_s gives SCORES\n",
    "    for all vocabulary elements at all timesteps, and y gives the INDICES of the\n",
    "    ground-truth element at each timestep. We use a cross-entropy loss at each\n",
    "    timestep, summing the loss over all timesteps and averaging across the minibatch.\n",
    "    \n",
    "    As an additional complication, we may want to ignore the model output at some\n",
    "    timesteps, since sequences of different length may have been combined into a\n",
    "    minibatch and padded with NULL tokens. The optional mask argument tells us\n",
    "    which elements should contribute to the loss.\n",
    "    \n",
    "    Inputs:\n",
    "    - y_s: Input scores, of shape (N, T, M)\n",
    "    - y: Ground-truth indices, of shape (N, T) where each element is in the range\n",
    "         0 <= y[i, t] < M\n",
    "    - mask: Boolean array of shape (N, T) where mask[i, t] tells whether or not\n",
    "      the scores at y_s[i, t] should contribute to the loss.\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving loss\n",
    "    - dy_s: Gradient of loss with respect to scores y_s, of shape (N, T, M)\n",
    "    \"\"\"\n",
    "    \n",
    "    N, T, M = y_s.shape\n",
    "\n",
    "    ys_flat = y_s.reshape(N * T, M)\n",
    "    y_flat = y.reshape(N * T)\n",
    "    mask_flat = mask.reshape(N * T)\n",
    "\n",
    "#     print(\"mask, mask_flat shapes = \", mask.shape, mask_flat.shape)\n",
    "#     print(\"ys, ys_flat shapes = \", y_s.shape, ys_flat.shape)\n",
    "#     print(\"y, y_flat shapes = \", y.shape, y_flat.shape)\n",
    "    \n",
    "    probs = np.exp(ys_flat - np.max(ys_flat, axis=1, keepdims=True))\n",
    "    probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "    loss = -np.sum(np.multiply(mask_flat, np.log(probs[np.arange(N * T), y_flat]))) / N\n",
    "    \n",
    "    dys_flat = probs.copy()\n",
    "    dys_flat[np.arange(N * T), y_flat] -= 1\n",
    "    dys_flat /= N\n",
    "    \n",
    "    mask_flat = mask_flat.reshape((-1, 1))\n",
    "#     print(dys_flat.shape, mask_flat.shape) #(928, 2128), (928,)\n",
    "    \n",
    "    dys_flat *= mask_flat\n",
    "    \n",
    "    if verbose: print('dys_flat: ', dys_flat.shape)\n",
    "\n",
    "    dy_s = dys_flat.reshape(N, T, M)\n",
    "\n",
    "    return loss, dy_s\n",
    "\n",
    "\n",
    "def softmax_loss(x, y, mask):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for softmax classification.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
    "      class for the ith input.\n",
    "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "      0 <= y[i] < C\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    shifted_logits = x - np.max(x, axis=1, keepdims=True) #(N,C)\n",
    "    Z = np.sum(np.exp(shifted_logits), axis=1, keepdims=True) #(N,1)\n",
    "    log_probs = shifted_logits - np.log(Z) #(N,C), log_probs_i=f_{y_i} + log(\\sigma_j e^{f_j})\n",
    "    probs = np.exp(log_probs) #(N,C)\n",
    "    N = x.shape[0]\n",
    "    loss = -np.sum(np.multiply(mask,log_probs[np.arange(N), y])) / N\n",
    "\n",
    "    dx = probs.copy()\n",
    "    dx[np.arange(N), y] -= 1\n",
    "    dx /= N\n",
    "    print(\"dx = \", dx, dx.shape)\n",
    "    return loss, dx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding_forward(x, W):\n",
    "    \"\"\"\n",
    "    Forward pass for word embeddings. We operate on minibatches of size N where\n",
    "    each sequence has length T. We assume a vocabulary of V words, assigning each\n",
    "    to a vector of dimension D.\n",
    "    Inputs:\n",
    "    - x: Integer array of shape (N, T) giving indices of words. Each element idx\n",
    "      of x muxt be in the range 0 <= idx < V.\n",
    "    - W: Weight matrix of shape (V, D) giving word vectors for all words.\n",
    "    Returns a tuple of:\n",
    "    - out: Array of shape (N, T, D) giving word vectors for all input words.\n",
    "    - cache: Values needed for the backward pass\n",
    "    \"\"\"\n",
    "    out = W[x, :] \n",
    "    cache = (x, W)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def word_embedding_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for word embeddings. We cannot back-propagate into the words\n",
    "    since they are integers, so we only return gradient for the word embedding\n",
    "    matrix.\n",
    "    Inputs:\n",
    "    - dout: Upstream gradients of shape (N, T, D)\n",
    "    - cache: Values from the forward pass\n",
    "    Returns:\n",
    "    - dW: Gradient of word embedding matrix, of shape (V, D).\n",
    "    \"\"\"\n",
    "    \n",
    "    # x: (N, T)\n",
    "    # W: (V, D)\n",
    "    x, W = cache\n",
    "    N, T, D = dout.shape\n",
    "\n",
    "    dW = np.zeros_like(W)\n",
    "    np.add.at(dW, x.reshape([-1]), dout.reshape([-1, D]))\n",
    "    \n",
    "    return dW\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier (Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data, we select: 60 train, 20 val, 20 test\n",
    "mode = 'word'\n",
    "\n",
    "if (mode=='char'): X_data = X_train_char\n",
    "else: X_data = X_train_word\n",
    "# X_train, X_val, X_test = np.split(X_data.sample(frac=1), [int(.6*len(X_data)), int(.8*len(X_data))])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(X_data, test_size=0.2, random_state=42)\n",
    "X_train, X_val = train_test_split(X_train, test_size=0.25, random_state=42)\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialisation\n",
    "#Here, some vars are repeated to memorize\n",
    "\n",
    "hidden_size = 200 # H, nb of neurons in hidden layer\n",
    "seq_length = 25 # T, nb of steps to unroll the RNN for <=> nb of chars putting in the input of RNN\n",
    "\n",
    "\n",
    "mode = \"word\"\n",
    "vocab_size = len(chars)+1 # D\n",
    "wordvec_size = vocab_size # V, if 'char' then D=V as we don't use wordvec\n",
    "data_size = X_train.shape[0] #X_train_char.shape[0]\n",
    "\n",
    "if mode==\"word\":\n",
    "#     data_size = X_train_word.shape[0] #len(sents)\n",
    "    vocab_size = len(word_to_index)\n",
    "    wordvec_size = 128 # output embedding vector length\n",
    "\n",
    "#init word/char vector\n",
    "W_embed = np.random.randn(vocab_size, wordvec_size)\n",
    "W_embed /=100\n",
    "\n",
    "\n",
    "#init RNN parameters\n",
    "W_xh = np.random.randn(wordvec_size, hidden_size) #(V,H) or (D,H)\n",
    "W_xh /= wordvec_size\n",
    "W_hh = np.random.randn(hidden_size, hidden_size) #(H,H)\n",
    "W_hh /= hidden_size\n",
    "W_hy = np.random.randn(hidden_size, vocab_size) #(H,M)\n",
    "W_hy /= hidden_size\n",
    "b_h = np.zeros((1, hidden_size)) #(H,)\n",
    "b_y = np.zeros((1, vocab_size)) #(M,)\n",
    "\n",
    "#init a dict of parameters\n",
    "params = {}\n",
    "params['W_embed'] = W_embed \n",
    "\n",
    "params['W_xh'], params['W_hh'], params['W_hy'], params['b_h'], params['b_y'] =  W_xh, W_hh, W_hy, b_h, b_y\n",
    "\n",
    "#reg: Scalar giving L2 regularization strength. If None then no reg is used\n",
    "reg=0.1\n",
    "\n",
    "\n",
    "\n",
    "def initParameter():\n",
    "\n",
    "    hidden_size = 200 # H, nb of neurons in hidden layer\n",
    "    seq_length = 25 # T, nb of steps to unroll the RNN for <=> nb of chars putting in the input of RNN\n",
    "\n",
    "\n",
    "    mode = \"char\"\n",
    "    vocab_size = len(chars)+1 # D\n",
    "    wordvec_size = vocab_size # V, if 'char' then D=V as we don't use wordvec\n",
    "    data_size = X_train_char.shape[0]\n",
    "\n",
    "    if mode==\"word\":\n",
    "        data_size = X_train_word.shape[0] #len(sents)\n",
    "        vocab_size = len(word_to_index)\n",
    "        wordvec_size = 128 # output embedding vector length\n",
    "\n",
    "    #init word/char vector\n",
    "    W_embed = np.random.randn(vocab_size, wordvec_size)\n",
    "    W_embed /=100\n",
    "\n",
    "\n",
    "    #init RNN parameters\n",
    "    W_xh = np.random.randn(wordvec_size, hidden_size) #(V,H) or (D,H)\n",
    "    W_xh /= wordvec_size\n",
    "    W_hh = np.random.randn(hidden_size, hidden_size) #(H,H)\n",
    "    W_hh /= hidden_size\n",
    "    W_hy = np.random.randn(hidden_size, vocab_size) #(H,M)\n",
    "    W_hy /= hidden_size\n",
    "    b_h = np.zeros((1, hidden_size)) #(H,)\n",
    "    b_y = np.zeros((1, vocab_size)) #(M,)\n",
    "\n",
    "    #init a dict of parameters\n",
    "    params = {}\n",
    "    params['W_embed'] = W_embed \n",
    "    params['W_xh'], params['W_hh'], params['W_hy'], params['b_h'], params['b_y'] =  W_xh, W_hh, W_hy, b_h, b_y\n",
    "    \n",
    "    #reg: Scalar giving L2 regularization strength. If None then no reg is used\n",
    "    reg=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of solver\n",
    "\"\"\"\n",
    "    - update_rule: A string giving the name of an update rule. Default is 'sgd'.\n",
    "    - opt_config: A dictionary containing hyperparameters that will be\n",
    "        passed to the chosen update rule. Each update rule requires different\n",
    "        hyperparameters, but all update rules require a 'learning_rate' parameter.\n",
    "    - lr_decay: A scalar for learning rate decay; after each epoch the\n",
    "        learning rate is multiplied by this value.\n",
    "    - batch_size: Size of minibatches used to compute loss and gradient\n",
    "        during training.\n",
    "    - num_epochs: The number of epochs to run for during training.\n",
    "    - print_every: Integer; training losses will be printed every\n",
    "        print_every iterations.\n",
    "    - verbose: Boolean; if set to false then no output will be printed\n",
    "        during training.\n",
    "    - num_train_samples: Number of training samples used to check training\n",
    "        accuracy; default is 1000; set to None to use entire training set.\n",
    "    - num_val_samples: Number of validation samples to use to check val\n",
    "        accuracy; default is None, which uses the entire validation set.\n",
    "    - checkpoint_name: If not None, then save model checkpoints here every\n",
    "        epoch.\n",
    "\"\"\"\n",
    "\n",
    "update_rule = 'sgd'\n",
    "opt_config={'learning_rate': 1e-2}\n",
    "lr_decay = 1.0\n",
    "batch_size = 2 # N\n",
    "num_epochs = 10 # E\n",
    "print_every = 100\n",
    "verbose = True\n",
    "num_train_samples = 1000\n",
    "num_val_samples = None\n",
    "checkpoint_name = None\n",
    "\n",
    "\n",
    "# Make a deep copy of the opt_config for each parameter\n",
    "optim_configs = {}\n",
    "for p in params:\n",
    "    print(p, params[p].shape)\n",
    "    optim_configs[p] = {k: v for k, v in opt_config.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update rules (Optimization methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optim:\n",
    "    \"\"\"\n",
    "    This class implements various first-order update rules that are commonly used\n",
    "    for training neural networks. Each update rule accepts current weights and the\n",
    "    gradient of the loss with respect to those weights and produces the next set of\n",
    "    weights. Each update rule has the same interface:\n",
    "\n",
    "    def update(w, dw, config=None):\n",
    "\n",
    "    Inputs:\n",
    "      - w: A numpy array giving the current weights.\n",
    "      - dw: A numpy array of the same shape as w giving the gradient of the\n",
    "        loss with respect to w.\n",
    "      - config: A dictionary containing hyperparameter values such as learning\n",
    "        rate, momentum, etc. If the update rule requires caching values over many\n",
    "        iterations, then config will also hold these cached values.\n",
    "\n",
    "    Returns:\n",
    "      - next_w: The next point after the update.\n",
    "      - config: The config dictionary to be passed to the next iteration of the\n",
    "        update rule.\n",
    "\n",
    "    NOTE: For most update rules, the default learning rate will probably not\n",
    "    perform well; however the default values of the other hyperparameters should\n",
    "    work well for a variety of different problems.\n",
    "\n",
    "    For efficiency, update rules may perform in-place updates, mutating w and\n",
    "    setting next_w equal to w.\n",
    "    \"\"\"\n",
    "\n",
    "    def sgd(self, w, dw, config=None):\n",
    "        \"\"\"\n",
    "        Vanilla SGD update rule.\n",
    "\n",
    "        config format:\n",
    "        - learning_rate: Scalar learning rate.\n",
    "        \"\"\"\n",
    "        if config is None: config = {}\n",
    "        config.setdefault('learning_rate', 1e-2)\n",
    "        next_w = 0\n",
    "        \n",
    "        next_w -= config['learning_rate'] * dw\n",
    "        return next_w, config\n",
    "\n",
    "\n",
    "    def adam(self, x, dx, config=None):\n",
    "        \"\"\"\n",
    "        Uses the Adam update rule, which incorporates moving averages of both the\n",
    "        gradient and its square and a bias correction term.\n",
    "\n",
    "        config format:\n",
    "        - learning_rate: Scalar learning rate.\n",
    "        - beta1: Decay rate for moving average of first moment of gradient.\n",
    "        - beta2: Decay rate for moving average of second moment of gradient.\n",
    "        - epsilon: Small scalar used for smoothing to avoid dividing by zero.\n",
    "        - m: Moving average of gradient.\n",
    "        - v: Moving average of squared gradient.\n",
    "        - t: Iteration number.\n",
    "        \"\"\"\n",
    "        if config is None: config = {}\n",
    "        config.setdefault('learning_rate', 1e-3)\n",
    "        config.setdefault('beta1', 0.9)\n",
    "        config.setdefault('beta2', 0.999)\n",
    "        config.setdefault('epsilon', 1e-8)\n",
    "        config.setdefault('m', np.zeros_like(x))\n",
    "        config.setdefault('v', np.zeros_like(x))\n",
    "        config.setdefault('t', 0)\n",
    "\n",
    "        next_x = None\n",
    "        beta1, beta2, eps = config['beta1'], config['beta2'], config['epsilon']\n",
    "        t, m, v = config['t'], config['m'], config['v']\n",
    "        m = beta1 * m + (1 - beta1) * dx\n",
    "        v = beta2 * v + (1 - beta2) * (dx * dx)\n",
    "        t += 1\n",
    "        alpha = config['learning_rate'] * np.sqrt(1 - beta2 ** t) / (1 - beta1 ** t)\n",
    "        x -= alpha * (m / (np.sqrt(v) + eps))\n",
    "        config['t'] = t\n",
    "        config['m'] = m\n",
    "        config['v'] = v\n",
    "        next_x = x\n",
    "\n",
    "        return next_x, config\n",
    "\n",
    "optim = Optim()\n",
    "#define function to update parameters\n",
    "updateParameter = getattr(optim, update_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trainingLoss(text, img_fea=None, training=True, mode='char'):\n",
    "    \"\"\"\n",
    "    Compute training-time loss.\n",
    "    Inputs:\n",
    "    - img_fea: Image features (used for captioning), of shape (N, D)\n",
    "    - text: Ground-truth texts (e.g., captions); an integer array of shape (N, T) where\n",
    "        each element is in the range 0 <= y[i, t] < M\n",
    "    Returns:\n",
    "        If training=True, then run a test-time forward pass of the model and return:\n",
    "        - scores: Array of shape (N, M) giving classification scores, where\n",
    "          scores[i, c] is the classification score for y[i] and the word/char with index c.\n",
    "\n",
    "        Else, then run a training-time forward and backward pass and\n",
    "        return a tuple of:\n",
    "        - loss: Scalar value giving the loss\n",
    "        - grads: Dictionary of gradients \n",
    "    \"\"\"\n",
    "    singleton = False\n",
    "    if text.ndim == 1:\n",
    "        singleton = True\n",
    "        text = text[None]\n",
    "    \n",
    "    # Divide 'text' into two pieces \n",
    "    text_in = text[:, :-1]\n",
    "    text_out = text[:, 1:]\n",
    "#     print(text_in.shape, text_out.shape) # (2, 9518) = N, T\n",
    "    \n",
    "    if mode=='word':  mask = (text_out != word_to_index[unknown_token])\n",
    "    else: mask = (text_out != char_to_index[unknown_char])\n",
    "    \n",
    "#     print(mask, mask.shape) #(2, 9518) = N, T\n",
    "\n",
    "\n",
    "    #prepare parameters\n",
    "    W_xh = params['W_xh']; W_hh = params['W_hh']; W_hy = params['W_hy']\n",
    "    b_h = params['b_h']; b_y = params['b_y']\n",
    "    W_embed = params['W_embed']\n",
    "    \n",
    "    \n",
    "#     print(\"shapes of parameters: \",W_xh.shape, W_hh.shape, W_hy.shape, b_h.shape, b_y.shape, W_embed.shape)\n",
    "    \n",
    "    ##### FORWARD STEPS ######\n",
    "    embedded_text, cache_word_embedding = None, None\n",
    "    \n",
    "    # Embed the input word captions.\n",
    "    embedded_text, cache_word_embedding = word_embedding_forward(text_in, W_embed)\n",
    "    \n",
    "#     print(embedded_text.shape) # (N, T, D) = (2, 9518, 47)\n",
    "\n",
    "    #RNN forward\n",
    "    h_prev = np.zeros((text.shape[0], hidden_size)) #(N, H) = (2,200)\n",
    "    h, y_s, cache_rnn = rnn_forward(embedded_text, h_prev, W_xh, W_hh, W_hy, b_h, b_y)\n",
    "    \n",
    "    if training==False:\n",
    "        return y_s\n",
    "    \n",
    "    loss, grads = 0.0, {}\n",
    "\n",
    "    #Softmax\n",
    "    loss, dy_s = temporal_softmax_loss(y_s, text_out, mask)\n",
    "    \n",
    "    ##### BACKWARD STEPS ######\n",
    "    \n",
    "    #Backprop dy_s to get gradients\n",
    "    dx, dh0, dWxh, dWhh, dWhy, dbh, dby = rnn_backward(dy_s, cache_rnn)\n",
    "    \n",
    "    grads['W_xh'], grads['W_hh'], grads['W_hy'], grads['b_h'], grads['b_y'] = dWxh, dWhh, dWhy, dbh, dby\n",
    "\n",
    "    \n",
    "    # Backprop dx to get gradient for word embedding weights.\n",
    "#     if (mode=='word'):\n",
    "    dW_embed = word_embedding_backward(dx, cache_word_embedding)\n",
    "    grads['W_embed'] = dW_embed\n",
    "    \n",
    "\n",
    "    if (reg!=None):\n",
    "        for key in ['W_xh', 'W_hh', 'W_hy', 'W_embed']:\n",
    "            loss += 0.5 * reg * np.linalg.norm(params[key])**2\n",
    "            grads[key] += reg * np.linalg.norm(params[key])**2\n",
    "    \n",
    "    return loss, grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_captions(captions, mode='word'):\n",
    "    singleton = False\n",
    "    if captions.ndim == 1:\n",
    "        singleton = True\n",
    "        captions = captions[None]\n",
    "    decoded = []\n",
    "    N, T = captions.shape\n",
    "    \n",
    "    if mode==\"word\":\n",
    "        for i in range(N):\n",
    "            words = []\n",
    "            for t in range(T):\n",
    "                word = index_to_word[captions[i, t]]\n",
    "                if word != unknown_token:\n",
    "                    words.append(word)\n",
    "                if word == sentence_end_token:\n",
    "                    break\n",
    "            decoded.append(' '.join(words))\n",
    "        if singleton:\n",
    "            decoded = decoded[0]\n",
    "    else:\n",
    "        for i in range(N):\n",
    "            charset = []\n",
    "            for t in range(T):\n",
    "                c = index_to_char[captions[i, t]]\n",
    "                if c != '$':\n",
    "                    charset.append(c)\n",
    "                if c == '<END>':\n",
    "                    break\n",
    "            decoded.append(' '.join(charset))\n",
    "        if singleton:\n",
    "            decoded = decoded[0]\n",
    "    return decoded\n",
    "\n",
    "def BLEU_score(gt_sent, sample_sent,mode):\n",
    "    \"\"\"\n",
    "    gt_sent: string, ground-truth caption\n",
    "    sample_sent: string, your model's predicted caption\n",
    "    Returns unigram BLEU score.\n",
    "    \"\"\"\n",
    "\n",
    "    reference = []\n",
    "    for sent in gt_sent:\n",
    "        for x in sent: \n",
    "            if mode=='word':\n",
    "                if (word_to_index[sentence_start_token]!=x and word_to_index[sentence_end_token]!=x \\\n",
    "                    and word_to_index[unknown_token]!=x): reference.append(x)\n",
    "            else:\n",
    "                if (char_to_index[unknown_char]!=x): reference.append(x)\n",
    "            \n",
    "    hypothesis = []\n",
    "    for sent in sample_sent:\n",
    "        for x in sent: \n",
    "            if mode=='word':\n",
    "                if (word_to_index[sentence_start_token]!=x and word_to_index[sentence_end_token]!=x \\\n",
    "                    and word_to_index[unknown_token]!=x): hypothesis.append(x)\n",
    "            else:\n",
    "                if (char_to_index[unknown_char]!=x): hypothesis.append(x)\n",
    "            \n",
    "    \n",
    "#     reference = [x for x in sent \n",
    "#                  if (sentence_end_token not in x and sentence_start_token not in x and unknown_token not in x)]\n",
    "#     hypothesis = [[x for x in sent \n",
    "#                   if (sentence_end_token not in x and sentence_start_token not in x and unknown_token not in x)]\n",
    "#                   for sent in sample_sent]\n",
    "    BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, weights = [1])\n",
    "    return BLEUscore\n",
    "\n",
    "\n",
    "def evaluate_model(gt_sents, sample_sents, mode='word'):\n",
    "    \"\"\"\n",
    "    Prints unigram BLEU score averaged over training and val examples.\n",
    "    \"\"\"\n",
    "    total_score = 0.0\n",
    "    for gt, sample in zip(gt_sents, sample_sents):\n",
    "        total_score += BLEU_score(gt, sample, mode)\n",
    "\n",
    "    BLEU_scores = total_score / len(sample_sents)\n",
    "        \n",
    "    print('Average BLEU score is ', BLEU_scores)\n",
    "    return BLEU_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(desired_len, mode='char', first_char=None):\n",
    "    \"\"\"\n",
    "    Generate a sentence (containing words/chars) with a given length\n",
    "    \"\"\"\n",
    "    sentence_str = []\n",
    "    if(mode=='char'):\n",
    "        if first_char==None: return None\n",
    "        print(first_char, char_to_index)\n",
    "         # We start the sentence with the start token\n",
    "        new_char = [first_char] # if not yet converted to index, then use: [char_to_index[first_char]]\n",
    "        # Repeat until we get an end token\n",
    "        while not new_char[-1] == char_to_index['.'] and len(new_char)<desired_len+1:\n",
    "            next_char_probs = trainingLoss(np.asarray(new_char), training=False) #get proposed words\n",
    "            print(next_char_probs)\n",
    "            sampled_char = char_to_index[unknown_char]\n",
    "            # We don't want to sample unknown chars\n",
    "            while sampled_char == char_to_index[unknown_char]:\n",
    "                samples = np.random.multinomial(1, next_char_probs[-1]) #create a sample with prob is the last word prob\n",
    "                sampled_char = np.argmax(samples)\n",
    "            new_char.append(sampled_char)\n",
    "        sentence_str = [index_to_char[x] for x in new_char]\n",
    "    else:    \n",
    "        # We start the sentence with the start token\n",
    "        new_sentence = [word_to_index[sentence_start_token]]\n",
    "        # Repeat until we get an end token\n",
    "        while not new_sentence[-1] == word_to_index[sentence_end_token] and len(new_sentence)<desired_len+1:\n",
    "            next_word_probs = sample(new_sentence, training=False) #get proposed words\n",
    "            sampled_word = word_to_index[unknown_token]\n",
    "            # We don't want to sample unknown words\n",
    "            while sampled_word == word_to_index[unknown_token]:\n",
    "                samples = np.random.multinomial(1, next_word_probs[-1]) #create a sample with prob is the last word prob\n",
    "                sampled_word = np.argmax(samples)\n",
    "            new_sentence.append(sampled_word)\n",
    "        sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str\n",
    " \n",
    "def sample(batch_size, mode='word', first_char=None, max_length=30):\n",
    "        \"\"\"\n",
    "        Run a test-time forward pass for the model, generate text string\n",
    "        Inputs:\n",
    "        - batch_size: size of batch \n",
    "        - first_char: if considering 'char level', then generate a string based on the first char\n",
    "        - max_length: maximum length T of generated text.\n",
    "        Returns:\n",
    "        - text: Array of shape (N, max_length) giving sampled string,\n",
    "          where each element is an integer in the range [0, V). The first element\n",
    "          of text should be the first sampled word, not the <START> token.\n",
    "        \"\"\"\n",
    "        \n",
    "        N=batch_size\n",
    "        \n",
    "        if (mode=='char'):   text = char_to_index[unknown_char] * np.ones((N, max_length), dtype=np.int32)\n",
    "        else: text = word_to_index[unknown_token] * np.ones((N, max_length), dtype=np.int32)\n",
    "#         print(\"text.shape=\", text.shape) # (N,T) = (2, 10)\n",
    "        \n",
    "        # Unpack parameters\n",
    "        W_embed = params['W_embed'] # (V,D)=(47,47)\n",
    "        W_xh, W_hh, W_hy, b_h, b_y = params['W_xh'], params['W_hh'], params['W_hy'], params['b_h'], params['b_y']\n",
    "\n",
    "        H = W_xh.shape[1]\n",
    "        cur_hidden_state = np.zeros((N,H))\n",
    "        \n",
    "        print(W_embed.shape, W_xh.shape, W_hh.shape, W_hy.shape, b_h.shape, b_y.shape, cur_hidden_state.shape)\n",
    "        #(47, 47) (47, 200) (200, 200) (200, 47) (1, 200) (1, 47) (47, 200)\n",
    "        \n",
    "        # Embed our start token, will broadcast to size N.\n",
    "        if (mode=='char'):\n",
    "            word_embed, _ = word_embedding_forward(first_char, W_embed)\n",
    "        else: word_embed, _ = word_embedding_forward(word_to_index[sentence_start_token], W_embed)\n",
    "        print(word_embed.shape) #(2, 1, 47)\n",
    "        \n",
    "        word_embed = np.reshape(word_embed,(-1, 1)).T\n",
    "        \n",
    "        # Sample max_length number of words.\n",
    "        for i in range(max_length):\n",
    "            cur_hidden_state, cur_scores, _ = rnn_forward_single_step(word_embed, cur_hidden_state, W_xh, W_hh, W_hy, b_h, b_y)\n",
    "#             print(cur_scores.shape) # (2,47)\n",
    "            # Find the highest value index and assign it to the correct place in text.\n",
    "            text[:,i] = np.argmax(cur_scores, axis=1)\n",
    "#             print(\"i=\",i, \"text[i]=\", text[:,i])\n",
    "                \n",
    "            # Embed the word produced for the next iteration.\n",
    "            word_embed, _ = word_embedding_forward(text[:, i], W_embed)\n",
    "\n",
    "#         print(text, type(text)) \n",
    "        return text\n",
    "\n",
    "def check_accuracy(X, num_samples=None, batch_size=2):\n",
    "    \"\"\"\n",
    "    Check accuracy of the model on the provided data.\n",
    "\n",
    "    Inputs:\n",
    "    - X: Array of data, of shape (N, D)\n",
    "    - num_samples: If not None, subsample the data and only test the model\n",
    "        on num_samples datapoints.\n",
    "    - batch_size: Split X and y into batches of this size to avoid using\n",
    "        too much memory.\n",
    "\n",
    "    Returns:\n",
    "    - acc: Scalar giving the fraction of instances that were correctly\n",
    "          classified by the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # subsample the data\n",
    "    N = X.shape[0]\n",
    "    if num_samples is not None and N > num_samples:\n",
    "        mask = np.random.choice(N, num_samples)\n",
    "        N = num_samples\n",
    "        X = X[mask] \n",
    "        \n",
    "\n",
    "    # Compute predictions in batches\n",
    "    num_batches = N // batch_size\n",
    "    if N % batch_size != 0:\n",
    "        num_batches += 1\n",
    "    \n",
    "    y_pred = []\n",
    "    y=[] #this is a special case when considering LM\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        y.append(X[start:end])\n",
    "        max_length=10 # T=10\n",
    "        if (mode=='char'):  new_sent_char = sample(batch_size, mode='char', first_char=X[start:end,0], max_length=max_length)\n",
    "        else: new_sent_char = sample(batch_size, mode='word', max_length=max_length)\n",
    "        y_pred.append(new_sent_char)\n",
    "    \n",
    "    print(len(y), len(y_pred)) #30, 30\n",
    "#     y_pred = np.hstack(y_pred) #put into stack horizontally\n",
    "#     acc = np.mean(y_pred == y)\n",
    "    BLUE_score = evaluate_model(y, y_pred)\n",
    "    print(BLUE_score)\n",
    "    return BLUE_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "(Iteration 1 / 300) loss: nan\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "30 30\n",
      "Average BLEU score is  0.0\n",
      "0.0\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "10 10\n",
      "Average BLEU score is  0.0\n",
      "0.0\n",
      "0.0\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "30 30\n",
      "Average BLEU score is  0.0\n",
      "0.0\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "10 10\n",
      "Average BLEU score is  0.0\n",
      "0.0\n",
      "0.0\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "30 30\n",
      "Average BLEU score is  0.0\n",
      "0.0\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "10 10\n",
      "Average BLEU score is  0.0\n",
      "0.0\n",
      "0.0\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "30 30\n",
      "Average BLEU score is  0.0\n",
      "0.0\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "10 10\n",
      "Average BLEU score is  0.0\n",
      "0.0\n",
      "0.0\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "(Iteration 101 / 300) loss: nan\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "30 30\n",
      "Average BLEU score is  0.0\n",
      "0.0\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "10 10\n",
      "Average BLEU score is  0.0\n",
      "0.0\n",
      "0.0\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "30 30\n",
      "Average BLEU score is  0.0\n",
      "0.0\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "10 10\n",
      "Average BLEU score is  0.0\n",
      "0.0\n",
      "0.0\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "30 30\n",
      "Average BLEU score is  0.0\n",
      "0.0\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "10 10\n",
      "Average BLEU score is  0.0\n",
      "0.0\n",
      "0.0\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "(Iteration 201 / 300) loss: nan\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "30 30\n",
      "Average BLEU score is  0.0\n",
      "0.0\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "10 10\n",
      "Average BLEU score is  0.0\n",
      "0.0\n",
      "0.0\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "30 30\n",
      "Average BLEU score is  0.0\n",
      "0.0\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "10 10\n",
      "Average BLEU score is  0.0\n",
      "0.0\n",
      "0.0\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "30 30\n",
      "Average BLEU score is  0.0\n",
      "0.0\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "10 10\n",
      "Average BLEU score is  0.0\n",
      "0.0\n",
      "0.0\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "mask, mask_flat shapes =  (2, 464) (928,)\n",
      "ys, ys_flat shapes =  (2, 464, 2128) (928, 2128)\n",
      "y, y_flat shapes =  (2, 464) (928,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "30 30\n",
      "Average BLEU score is  0.0\n",
      "0.0\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "(2128, 128) (128, 200) (200, 200) (200, 2128) (1, 200) (1, 2128) (2, 200)\n",
      "(128,)\n",
      "10 10\n",
      "Average BLEU score is  0.0\n",
      "0.0\n",
      "0.0\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "loss_history = [] # a list for storing history of training loss\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "\n",
    "# Set up some variables for storing info of training\n",
    "epoch = 0\n",
    "best_val_acc = 0\n",
    "best_params = {}\n",
    "loss_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "def executeStep():\n",
    "    \"\"\"\n",
    "    Execute one iteration of training\n",
    "    \"\"\"\n",
    "    #sample data\n",
    "    N = X_train.shape[0]\n",
    "    batch_mask = np.random.choice(N, batch_size)\n",
    "    \n",
    "\n",
    "    #convert text into interger arrays\n",
    "    if (mode=='char'): \n",
    "        X_batch = X_train_char[batch_mask]\n",
    "    else: X_batch = X_train_word[batch_mask]\n",
    "    \n",
    "    \n",
    "    #compute loss for this batch\n",
    "    loss, grads = trainingLoss(X_batch)\n",
    "#     smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    \n",
    "    #add to history\n",
    "    loss_history.append(loss)\n",
    "\n",
    "    #update classifier (model) parameters\n",
    "    for w_name, old_w in params.items():\n",
    "        #get the gradient\n",
    "        dw = grads[w_name]\n",
    "        #get the updates\n",
    "        \n",
    "        next_w, new_config = updateParameter(old_w, dw, optim_configs[w_name]) #old value, old gradient, config\n",
    "        \n",
    "        #re-assign to params and opt_config\n",
    "        params[w_name] = next_w\n",
    "        optim_configs[w_name] = new_config\n",
    "        \n",
    "#         print(\"parameter \", w_name, \": oldval=\", old_w.shape, \"newval=\", next_w.shape)\n",
    "        \n",
    "#     return smooth_loss\n",
    "    \n",
    "def train():\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    num_train = data_size \n",
    "    iterations_per_epoch = max (num_train//batch_size, 1)\n",
    "    num_iterations = num_epochs * iterations_per_epoch\n",
    "    smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "    best_val_acc = 0\n",
    "    epoch=0\n",
    "    best_params = {}\n",
    "\n",
    "    \n",
    "    initParameter()\n",
    "    for t in range(num_iterations):\n",
    "            #Execute one step of training\n",
    "#             print(smooth_loss)\n",
    "#             smooth_loss = executeStep(smooth_loss)\n",
    "            executeStep()\n",
    "            # print loss for debug \n",
    "            if verbose and t % print_every == 0:\n",
    "                print('(Iteration %d / %d) loss: %f' % (\n",
    "                       t + 1, num_iterations, loss_history[-1]))\n",
    "\n",
    "            # Increment the epoch counter and decay the learning rate.\n",
    "            epoch_end = (t + 1) % iterations_per_epoch == 0\n",
    "            if epoch_end:\n",
    "                epoch += 1\n",
    "                for k in optim_configs:\n",
    "                    optim_configs[k]['learning_rate'] *= lr_decay\n",
    "\n",
    "            # Check train and val accuracy on the first iteration, the last\n",
    "            # iteration, and at the end of each epoch.\n",
    "            if (t==0 or t==num_iterations-1 or epoch_end):\n",
    "                train_acc = check_accuracy(X_train, num_samples=num_train_samples)\n",
    "                train_acc_history.append(train_acc)\n",
    "                \n",
    "                val_acc = check_accuracy(X_val, num_samples=num_val_samples)\n",
    "                val_acc_history.append(val_acc)\n",
    "                print(val_acc)\n",
    "                \n",
    "                if (val_acc > best_val_acc): \n",
    "                    #save the current best params\n",
    "                    best_val_acc = val_acc\n",
    "                    best_params = {}\n",
    "                    for k, v in params.items(): best_params[k] =  v.copy()\n",
    "                    \n",
    "\n",
    "    # swap the best params into the model\n",
    "    params = best_params\n",
    "    print(best_params)\n",
    "        \n",
    "        \n",
    "        \n",
    "#     n, p = 0, 0\n",
    "#     mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "#     mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "#     smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#     while True:\n",
    "#         # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "#         if p+seq_length+1 >= len(data) or n == 0: \n",
    "#             hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "#             p = 0 # go from start of data\n",
    "#         inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "#         targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "#           # sample from the model now and then\n",
    "#           if n % 100 == 0:\n",
    "#             sample_ix = sample(hprev, inputs[0], 200)\n",
    "#             txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "#             print '----\\n %s \\n----' % (txt, )\n",
    "\n",
    "#           # forward seq_length characters through the net and fetch gradient\n",
    "#           loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "#           smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "#           if n % 100 == 0: print 'iter %d, loss: %f' % (n, smooth_loss) # print progress\n",
    "\n",
    "#           # perform parameter update with Adagrad\n",
    "#           for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "#                                         [dWxh, dWhh, dWhy, dbh, dby], \n",
    "#                                         [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "#             mem += dparam * dparam\n",
    "#             param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad updatecheck_accuracy\n",
    "\n",
    "#           p += seq_length # move data pointer\n",
    "#         n += 1 # iteration counter \n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
